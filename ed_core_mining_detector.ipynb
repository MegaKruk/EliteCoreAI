{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585d2fa8",
   "metadata": {},
   "source": [
    "# Elite Dangerous Core Mining - ML Detection Pipeline\n",
    "\n",
    "This notebook trains YOLOv8 object-detection models to identify core asteroids\n",
    "in Elite Dangerous screenshots in real time.\n",
    "\n",
    "## Labeling tool recommendation\n",
    "\n",
    "Before you can train, you need labeled screenshots - bounding boxes drawn around\n",
    "every core asteroid in each image. The best free tools for this:\n",
    "\n",
    "**Recommended: [Roboflow Annotate](https://roboflow.com)** (web-based)\n",
    "- Free tier supports unlimited public projects and up to 10k images\n",
    "- Drag-and-drop upload, clean bounding box UI, keyboard shortcuts\n",
    "- Exports directly in YOLO format (what we need) or COCO JSON\n",
    "- Also does automatic dataset splitting (train/val) and augmentation previews\n",
    "- Easiest to share with others if you want help labeling\n",
    "\n",
    "**Local alternative: [LabelImg](https://github.com/HumanSignal/labelImg)**\n",
    "- Runs on Windows, totally offline, no account needed\n",
    "- `pip install labelImg` then run `labelImg` from command line\n",
    "- Save in YOLO format directly (one .txt per image, same filename as the image)\n",
    "- Keyboard shortcut `W` to draw a box, `D` for next image - fast once you get going\n",
    "\n",
    "**Workflow:**\n",
    "1. Play Elite Dangerous and take screenshots (F10 by default, saves to\n",
    "   `%USERPROFILE%\\Pictures\\Frontier Developments\\Elite Dangerous`)\n",
    "2. Label each screenshot - draw a bounding box around every core you can see\n",
    "   on screen (including ones the PWA is highlighting)\n",
    "3. Export as YOLO format and drop files into the dataset folders this notebook creates\n",
    "4. Run the retrain cell\n",
    "\n",
    "## Dataset structure this notebook expects\n",
    "\n",
    "```\n",
    "datasets/\n",
    "  ice/\n",
    "    images/\n",
    "      train/     <- ~80% of your labeled screenshots\n",
    "      val/       <- ~20% for validation\n",
    "    labels/\n",
    "      train/     <- one .txt per image, same name as the image\n",
    "      val/\n",
    "  metallic/\n",
    "    images/ ...\n",
    "    labels/ ...\n",
    "  metal-rich/\n",
    "    images/ ...\n",
    "    labels/ ...\n",
    "  rocky/\n",
    "    images/ ...\n",
    "    labels/ ...\n",
    "  unified/       <- copy of all images+labels combined (optional)\n",
    "    images/ ...\n",
    "    labels/ ...\n",
    "```\n",
    "\n",
    "Each label .txt file has one line per core:\n",
    "`0 <cx> <cy> <w> <h>` - class index (always 0 = core), then normalized center-x,\n",
    "center-y, width, height (all 0.0-1.0). Roboflow and LabelImg both produce this\n",
    "format automatically when you pick \"YOLO\" as the export format.\n",
    "\n",
    "## Strategy\n",
    "\n",
    "We train one model per ring type (ice, metallic, rocky, metal-rich) because:\n",
    "- Each ring type has exactly one asteroid shape that can contain a core\n",
    "- You mine one ring type per session, so you only need one model loaded at a time\n",
    "- Specialist models outperform a single multi-type model here\n",
    "\n",
    "We also train a \"unified\" model on all types combined as a comparison baseline.\n",
    "\n",
    "For each ring type we compare three YOLOv8 sizes (nano, small, medium) and keep\n",
    "the one with the best mAP50. The RTX 3070 (8GB VRAM) handles all three sizes\n",
    "comfortably at batch=16, img_size=640.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f32db",
   "metadata": {},
   "source": [
    "## 1. Install dependencies\n",
    "\n",
    "Run once. Restart the kernel after if prompted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740ab8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages installed.\n"
     ]
    }
   ],
   "source": [
    "# ultralytics gives us YOLOv8 + training loop + export tools\n",
    "# opencv-python for image loading and annotation drawing\n",
    "# pyyaml for reading/writing data.yaml config files\n",
    "# matplotlib for result visualization\n",
    "\n",
    "import subprocess, sys\n",
    "\n",
    "packages = [\n",
    "    \"ultralytics>=8.0.0\",\n",
    "    \"opencv-python\",\n",
    "    \"pyyaml\",\n",
    "    \"matplotlib\",\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "]\n",
    "\n",
    "subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\"] + packages,\n",
    "    check=True,\n",
    ")\n",
    "print(\"All packages installed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efce3ff",
   "metadata": {},
   "source": [
    "## 2. Imports and GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a0633b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\megakruk\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Python:  3.12.4 (tags/v3.12.4:8e8a4ba, Jun  6 2024, 19:30:16) [MSC v.1940 64 bit (AMD64)]\n",
      "PyTorch: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3070\n",
      "VRAM: 8.0 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import yaml\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print(f\"Python:  {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.cuda.get_device_properties(0)\n",
    "    vram_gb = gpu.total_memory / 1024 ** 3\n",
    "    print(f\"GPU: {gpu.name}\")\n",
    "    print(f\"VRAM: {vram_gb:.1f} GB\")\n",
    "    if vram_gb < 6:\n",
    "        print(\"WARNING: less than 6 GB VRAM - reduce BATCH_SIZE to 8 if training crashes\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU found. Training on CPU will be very slow.\")\n",
    "    print(\"Make sure CUDA toolkit is installed and torch was installed with CUDA support.\")\n",
    "    print(\"Install command: pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1078435",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "All tunable settings live here. Edit before running training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1421b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- MAIN CONFIG - edit this before running anything ----\n",
    "\n",
    "# Ring types to train models for.\n",
    "# \"unified\" trains on all types combined as a baseline comparison.\n",
    "# Remove any type you don't have data for yet.\n",
    "RING_TYPES = [\"ice\", \"metallic\", \"rocky\", \"metal-rich\", \"unified\"]\n",
    "\n",
    "# Model sizes to compare per ring type.\n",
    "# n=fastest/smallest, s=good balance, m=best accuracy but slower.\n",
    "# All three fit on RTX 3070 8GB at batch=16.\n",
    "MODEL_SIZES = [\"yolov8n\", \"yolov8s\", \"yolov8m\"]\n",
    "\n",
    "# Training hyperparameters\n",
    "IMG_SIZE    = 640   # YOLO standard input size\n",
    "EPOCHS      = 100   # how many epochs to train (early stopping kicks in via PATIENCE)\n",
    "BATCH_SIZE  = 16    # lower to 8 if you get CUDA out-of-memory errors\n",
    "PATIENCE    = 20    # stop training if val mAP doesn't improve for this many epochs\n",
    "\n",
    "# Confidence threshold for inference (0.0 - 1.0).\n",
    "# Lower = more detections (more false positives).\n",
    "# Higher = fewer, more confident detections (might miss some).\n",
    "# Start at 0.4 and tune based on how the model performs in-game.\n",
    "CONF_THRESHOLD = 0.4\n",
    "\n",
    "# We only detect one thing: a core asteroid.\n",
    "# YOLO treats everything else as background automatically.\n",
    "CLASS_NAMES = [\"core\"]\n",
    "\n",
    "# Folder layout\n",
    "DATASET_ROOT = Path(\"datasets\")\n",
    "RUNS_ROOT    = Path(\"runs\")\n",
    "EXPORTS_ROOT = Path(\"exports\")\n",
    "\n",
    "print(\"Config loaded.\")\n",
    "print(f\"Ring types: {RING_TYPES}\")\n",
    "print(f\"Model sizes: {MODEL_SIZES}\")\n",
    "print(f\"Training on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bea17c",
   "metadata": {},
   "source": [
    "## 4. Create dataset folder structure\n",
    "\n",
    "Run this once. Then drop your labeled screenshots into the folders it prints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_structure():\n",
    "    \"\"\"Create train/val image and label folders for each ring type.\"\"\"\n",
    "    for ring_type in RING_TYPES:\n",
    "        for split in [\"train\", \"val\"]:\n",
    "            (DATASET_ROOT / ring_type / \"images\" / split).mkdir(parents=True, exist_ok=True)\n",
    "            (DATASET_ROOT / ring_type / \"labels\" / split).mkdir(parents=True, exist_ok=True)\n",
    "    RUNS_ROOT.mkdir(exist_ok=True)\n",
    "    EXPORTS_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "    print(\"Folder structure created. Drop your screenshots and labels here:\\n\")\n",
    "    for ring_type in RING_TYPES:\n",
    "        print(f\"  datasets/{ring_type}/images/train/  <- training screenshots (.png or .jpg)\")\n",
    "        print(f\"  datasets/{ring_type}/images/val/    <- validation screenshots (about 20% of total)\")\n",
    "        print(f\"  datasets/{ring_type}/labels/train/  <- matching YOLO .txt label files\")\n",
    "        print(f\"  datasets/{ring_type}/labels/val/\")\n",
    "        print()\n",
    "\n",
    "create_dataset_structure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f872bfa",
   "metadata": {},
   "source": [
    "## 5. COCO JSON to YOLO format converter\n",
    "\n",
    "If your labeling tool exports COCO JSON instead of YOLO .txt files, run this.\n",
    "Roboflow can export either format - YOLO is preferred and skips this step entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e9959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coco_to_yolo(coco_json_path, output_labels_dir):\n",
    "    \"\"\"\n",
    "    Convert a COCO-format annotations JSON to per-image YOLO .txt label files.\n",
    "\n",
    "    COCO format: one big JSON file with all annotations\n",
    "    YOLO format: one .txt file per image, each line = class cx cy w h (normalized 0-1)\n",
    "\n",
    "    Args:\n",
    "        coco_json_path: path to the _annotations.coco.json file\n",
    "        output_labels_dir: folder where .txt files will be written\n",
    "    \"\"\"\n",
    "    output_labels_dir = Path(output_labels_dir)\n",
    "    output_labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(coco_json_path) as f:\n",
    "        coco = json.load(f)\n",
    "\n",
    "    # map COCO category ids to our YOLO class indices\n",
    "    cat_map = {}\n",
    "    for cat in coco[\"categories\"]:\n",
    "        if cat[\"name\"] in CLASS_NAMES:\n",
    "            cat_map[cat[\"id\"]] = CLASS_NAMES.index(cat[\"name\"])\n",
    "\n",
    "    if not cat_map:\n",
    "        print(f\"WARNING: none of your COCO categories match CLASS_NAMES {CLASS_NAMES}\")\n",
    "        print(f\"Found categories: {[c['name'] for c in coco['categories']]}\")\n",
    "        return 0\n",
    "\n",
    "    images = {img[\"id\"]: img for img in coco[\"images\"]}\n",
    "\n",
    "    anns_by_image = defaultdict(list)\n",
    "    for ann in coco[\"annotations\"]:\n",
    "        if ann[\"category_id\"] in cat_map:\n",
    "            anns_by_image[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "    converted = 0\n",
    "    for img_id, anns in anns_by_image.items():\n",
    "        img_info = images[img_id]\n",
    "        W = img_info[\"width\"]\n",
    "        H = img_info[\"height\"]\n",
    "        img_name = Path(img_info[\"file_name\"]).stem\n",
    "\n",
    "        lines = []\n",
    "        for ann in anns:\n",
    "            cls = cat_map[ann[\"category_id\"]]\n",
    "            x, y, w, h = ann[\"bbox\"]  # COCO: top-left x,y + pixel width,height\n",
    "            cx = (x + w / 2) / W\n",
    "            cy = (y + h / 2) / H\n",
    "            nw = w / W\n",
    "            nh = h / H\n",
    "            lines.append(f\"{cls} {cx:.6f} {cy:.6f} {nw:.6f} {nh:.6f}\")\n",
    "\n",
    "        out_path = output_labels_dir / f\"{img_name}.txt\"\n",
    "        out_path.write_text(\"\\n\".join(lines))\n",
    "        converted += 1\n",
    "\n",
    "    print(f\"Converted {converted} images.\")\n",
    "    print(f\"Labels written to: {output_labels_dir}\")\n",
    "    return converted\n",
    "\n",
    "\n",
    "# Example usage - uncomment and adjust paths:\n",
    "# convert_coco_to_yolo(\n",
    "#     coco_json_path=\"my_export/_annotations.coco.json\",\n",
    "#     output_labels_dir=\"datasets/ice/labels/train\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b510da19",
   "metadata": {},
   "source": [
    "## 6. Validate datasets\n",
    "\n",
    "Run this after adding your screenshots and labels. Catches missing files, bad label format, and shows class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4116cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset(ring_type):\n",
    "    \"\"\"\n",
    "    Check a ring type dataset for problems before training.\n",
    "    Returns (stats dict, list of issues). Empty issues list means all clear.\n",
    "    \"\"\"\n",
    "    ring_dir = DATASET_ROOT / ring_type\n",
    "    issues = []\n",
    "    stats = {}\n",
    "\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        img_dir = ring_dir / \"images\" / split\n",
    "        lbl_dir = ring_dir / \"labels\" / split\n",
    "\n",
    "        images = list(img_dir.glob(\"*.png\")) + list(img_dir.glob(\"*.jpg\"))\n",
    "        labels = list(lbl_dir.glob(\"*.txt\"))\n",
    "\n",
    "        img_stems = {p.stem for p in images}\n",
    "        lbl_stems = {p.stem for p in labels}\n",
    "\n",
    "        missing_labels = img_stems - lbl_stems\n",
    "        orphan_labels  = lbl_stems - img_stems\n",
    "\n",
    "        if missing_labels:\n",
    "            issues.append(\n",
    "                f\"[{split}] {len(missing_labels)} image(s) have no label file: \"\n",
    "                + str(sorted(missing_labels)[:5])\n",
    "            )\n",
    "        if orphan_labels:\n",
    "            issues.append(\n",
    "                f\"[{split}] {len(orphan_labels)} label file(s) have no matching image: \"\n",
    "                + str(sorted(orphan_labels)[:5])\n",
    "            )\n",
    "\n",
    "        total_cores = 0\n",
    "        bad_lines = 0\n",
    "        for lbl in labels:\n",
    "            for line in lbl.read_text().strip().splitlines():\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split()\n",
    "                if len(parts) != 5:\n",
    "                    bad_lines += 1\n",
    "                    continue\n",
    "                try:\n",
    "                    cls, cx, cy, w, h = int(parts[0]), float(parts[1]), float(parts[2]), float(parts[3]), float(parts[4])\n",
    "                    if not (0 <= cx <= 1 and 0 <= cy <= 1 and 0 < w <= 1 and 0 < h <= 1):\n",
    "                        bad_lines += 1\n",
    "                    elif cls == 0:\n",
    "                        total_cores += 1\n",
    "                except ValueError:\n",
    "                    bad_lines += 1\n",
    "\n",
    "        if bad_lines:\n",
    "            issues.append(f\"[{split}] {bad_lines} malformed label line(s)\")\n",
    "\n",
    "        stats[split] = {\n",
    "            \"images\": len(images),\n",
    "            \"labeled\": len(img_stems & lbl_stems),\n",
    "            \"cores\": total_cores,\n",
    "        }\n",
    "\n",
    "    print(f\"\\n{'=' * 40}\")\n",
    "    print(f\"  {ring_type.upper()} dataset\")\n",
    "    print(f\"{'=' * 40}\")\n",
    "    for split, s in stats.items():\n",
    "        ratio = s[\"cores\"] / s[\"labeled\"] if s[\"labeled\"] else 0\n",
    "        print(f\"  {split:5}: {s['images']:4} images | {s['cores']:5} core annotations | {ratio:.1f} cores/image\")\n",
    "\n",
    "    if issues:\n",
    "        print(\"  ISSUES:\")\n",
    "        for iss in issues:\n",
    "            print(f\"    - {iss}\")\n",
    "    else:\n",
    "        print(\"  All checks passed.\")\n",
    "\n",
    "    return stats, issues\n",
    "\n",
    "\n",
    "# Run validation for all ring types\n",
    "for ring_type in RING_TYPES:\n",
    "    validate_dataset(ring_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e82eb34",
   "metadata": {},
   "source": [
    "## 7. Preview labeled images\n",
    "\n",
    "Sanity check - visually confirm your labels look correct before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_labels(ring_type, split=\"train\", n=4):\n",
    "    \"\"\"\n",
    "    Show the first n labeled images with bounding boxes drawn on them.\n",
    "    Use this to confirm your labels were imported correctly.\n",
    "    \"\"\"\n",
    "    img_dir = DATASET_ROOT / ring_type / \"images\" / split\n",
    "    lbl_dir = DATASET_ROOT / ring_type / \"labels\" / split\n",
    "\n",
    "    images = sorted(list(img_dir.glob(\"*.png\")) + list(img_dir.glob(\"*.jpg\")))[:n]\n",
    "\n",
    "    if not images:\n",
    "        print(f\"No images found in {img_dir}\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(5 * len(images), 5))\n",
    "    if len(images) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, img_path in zip(axes, images):\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        H, W = img.shape[:2]\n",
    "\n",
    "        lbl_path = lbl_dir / (img_path.stem + \".txt\")\n",
    "        if lbl_path.exists():\n",
    "            for line in lbl_path.read_text().strip().splitlines():\n",
    "                parts = line.split()\n",
    "                if len(parts) != 5:\n",
    "                    continue\n",
    "                _, cx, cy, w, h = float(parts[0]), float(parts[1]), float(parts[2]), float(parts[3]), float(parts[4])\n",
    "                x1 = int((cx - w / 2) * W)\n",
    "                y1 = int((cy - h / 2) * H)\n",
    "                x2 = int((cx + w / 2) * W)\n",
    "                y2 = int((cy + h / 2) * H)\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(img, \"core\", (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(img_path.name, fontsize=8)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(f\"{ring_type} / {split} - label preview\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Change ring_type to whichever dataset you want to inspect\n",
    "preview_labels(\"ice\", split=\"train\", n=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7521e3",
   "metadata": {},
   "source": [
    "## 8. Generate data.yaml files\n",
    "\n",
    "YOLO needs a small YAML config file that points to the dataset folders and lists class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3ea884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_yaml(ring_type):\n",
    "    \"\"\"Write the data.yaml file for a ring type dataset.\"\"\"\n",
    "    ring_dir = DATASET_ROOT / ring_type\n",
    "    yaml_path = ring_dir / \"data.yaml\"\n",
    "\n",
    "    config = {\n",
    "        \"path\": str(ring_dir.resolve()),\n",
    "        \"train\": \"images/train\",\n",
    "        \"val\": \"images/val\",\n",
    "        \"nc\": len(CLASS_NAMES),\n",
    "        \"names\": CLASS_NAMES,\n",
    "    }\n",
    "\n",
    "    with open(yaml_path, \"w\") as f:\n",
    "        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "    print(f\"Written: {yaml_path}\")\n",
    "    return yaml_path\n",
    "\n",
    "\n",
    "for ring_type in RING_TYPES:\n",
    "    generate_yaml(ring_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaac382c",
   "metadata": {},
   "source": [
    "## 9. Train models\n",
    "\n",
    "This trains every combination of ring type x model size. Expect:\n",
    "- yolov8n: ~5-10 min per 100 epochs on RTX 3070\n",
    "- yolov8s: ~10-20 min\n",
    "- yolov8m: ~20-40 min\n",
    "\n",
    "Results are saved under `runs/<ring_type>_<model_size>_<timestamp>/`.\n",
    "The best weights checkpoint is at `weights/best.pt` inside that folder.\n",
    "\n",
    "If you don't have data for a ring type yet, that type is skipped automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32faa405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(ring_type, model_size):\n",
    "    \"\"\"\n",
    "    Train one YOLOv8 model for a given ring type and model size.\n",
    "    Returns (weights_path, results) or None if skipped.\n",
    "    \"\"\"\n",
    "    yaml_path = DATASET_ROOT / ring_type / \"data.yaml\"\n",
    "    if not yaml_path.exists():\n",
    "        print(f\"No data.yaml for {ring_type} - run generate_yaml first.\")\n",
    "        return None\n",
    "\n",
    "    train_images = (\n",
    "        list((DATASET_ROOT / ring_type / \"images\" / \"train\").glob(\"*.png\"))\n",
    "        + list((DATASET_ROOT / ring_type / \"images\" / \"train\").glob(\"*.jpg\"))\n",
    "    )\n",
    "    if len(train_images) < 5:\n",
    "        print(f\"Skipping {ring_type}/{model_size}: only {len(train_images)} training images (need >= 5).\")\n",
    "        return None\n",
    "\n",
    "    run_name = f\"{ring_type}_{model_size}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    print(f\"\\nStarting: {run_name}  ({len(train_images)} training images)\")\n",
    "\n",
    "    model = YOLO(f\"{model_size}.pt\")  # downloads pretrained COCO weights on first run\n",
    "\n",
    "    results = model.train(\n",
    "        data=str(yaml_path),\n",
    "        epochs=EPOCHS,\n",
    "        imgsz=IMG_SIZE,\n",
    "        batch=BATCH_SIZE,\n",
    "        patience=PATIENCE,\n",
    "        device=0 if torch.cuda.is_available() else \"cpu\",\n",
    "        project=str(RUNS_ROOT),\n",
    "        name=run_name,\n",
    "\n",
    "        # -- Augmentation tuned for Elite Dangerous screenshots --\n",
    "        # Asteroids rotate constantly and are viewed from any angle\n",
    "        degrees=45,\n",
    "        # Horizontal flip is always geometrically valid\n",
    "        fliplr=0.5,\n",
    "        # Vertical flip less common but fine for 3D space\n",
    "        flipud=0.3,\n",
    "        # Cores appear at various distances = various on-screen sizes\n",
    "        scale=0.5,\n",
    "        # Mosaic helps the model learn cluttered ring environments\n",
    "        mosaic=1.0,\n",
    "        close_mosaic=10,\n",
    "        # Brightness and saturation shift for lighting variation\n",
    "        # (shadow side of planet vs star-lit area)\n",
    "        hsv_v=0.4,\n",
    "        hsv_s=0.5,\n",
    "        # Small hue shift - ring tones vary but not wildly\n",
    "        hsv_h=0.015,\n",
    "        # Game renders are crisp - don't add blur\n",
    "        blur=False,\n",
    "        median_blur=0.0,\n",
    "\n",
    "        save_period=10,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    best_weights = RUNS_ROOT / run_name / \"weights\" / \"best.pt\"\n",
    "    map50 = results.results_dict.get(\"metrics/mAP50(B)\", 0)\n",
    "    print(f\"Done: {run_name}  mAP50={map50:.3f}  weights -> {best_weights}\")\n",
    "    return best_weights, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769ed481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all ring type x model size combinations.\n",
    "# Skips any ring type that doesn't have enough data yet.\n",
    "# This is the main training cell - takes a while.\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for ring_type in RING_TYPES:\n",
    "    stats, issues = validate_dataset(ring_type)\n",
    "    if issues:\n",
    "        print(f\"Skipping {ring_type} - fix dataset issues first.\")\n",
    "        continue\n",
    "\n",
    "    total_train = stats.get(\"train\", {}).get(\"images\", 0)\n",
    "    if total_train < 5:\n",
    "        print(f\"Skipping {ring_type} - not enough images yet ({total_train}).\")\n",
    "        continue\n",
    "\n",
    "    all_results[ring_type] = {}\n",
    "\n",
    "    for model_size in MODEL_SIZES:\n",
    "        result = train_model(ring_type, model_size)\n",
    "        if result is None:\n",
    "            continue\n",
    "\n",
    "        weights_path, train_results = result\n",
    "        rd = train_results.results_dict\n",
    "\n",
    "        all_results[ring_type][model_size] = {\n",
    "            \"weights\":    str(weights_path),\n",
    "            \"mAP50\":      float(rd.get(\"metrics/mAP50(B)\",     0)),\n",
    "            \"mAP50_95\":   float(rd.get(\"metrics/mAP50-95(B)\",  0)),\n",
    "            \"precision\":  float(rd.get(\"metrics/precision(B)\", 0)),\n",
    "            \"recall\":     float(rd.get(\"metrics/recall(B)\",    0)),\n",
    "        }\n",
    "\n",
    "# Persist results so retrain() can pick up where we left off\n",
    "results_path = RUNS_ROOT / \"all_results.json\"\n",
    "RUNS_ROOT.mkdir(exist_ok=True)\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(\"\\nAll training runs complete. Results saved to:\", results_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fb4792",
   "metadata": {},
   "source": [
    "## 10. Compare model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa05a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_comparison_table():\n",
    "    \"\"\"Print a formatted table of all trained model metrics.\"\"\"\n",
    "    results_path = RUNS_ROOT / \"all_results.json\"\n",
    "    if not results_path.exists():\n",
    "        print(\"No results yet - run training first.\")\n",
    "        return {}\n",
    "\n",
    "    with open(results_path) as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "    header = f\"{'Ring':<12} {'Model':<12} {'mAP50':>8} {'mAP50-95':>10} {'Precision':>10} {'Recall':>8}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    best_per_ring = {}\n",
    "    for ring_type, models in results.items():\n",
    "        best_map = -1\n",
    "        for model_size, m in models.items():\n",
    "            marker = \"\"\n",
    "            if m[\"mAP50\"] > best_map:\n",
    "                best_map = m[\"mAP50\"]\n",
    "                best_per_ring[ring_type] = model_size\n",
    "                marker = \" <-- best\"\n",
    "            print(\n",
    "                f\"{ring_type:<12} {model_size:<12} \"\n",
    "                f\"{m['mAP50']:>8.3f} {m['mAP50_95']:>10.3f} \"\n",
    "                f\"{m['precision']:>10.3f} {m['recall']:>8.3f}\"\n",
    "                + marker\n",
    "            )\n",
    "        print()\n",
    "\n",
    "    return best_per_ring\n",
    "\n",
    "\n",
    "best_models = show_comparison_table()\n",
    "print(\"Best model per ring type:\", best_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae9cf9",
   "metadata": {},
   "source": [
    "## 11. Plot training curves\n",
    "\n",
    "YOLOv8 saves a `results.csv` in each run folder. This plots loss and mAP over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def plot_training_curves(ring_type, model_size):\n",
    "    \"\"\"Plot loss and mAP curves from a training run's results.csv.\"\"\"\n",
    "    results_path = RUNS_ROOT / \"all_results.json\"\n",
    "    if not results_path.exists():\n",
    "        print(\"No training results found.\")\n",
    "        return\n",
    "\n",
    "    with open(results_path) as f:\n",
    "        all_r = json.load(f)\n",
    "\n",
    "    weights = all_r.get(ring_type, {}).get(model_size, {}).get(\"weights\", \"\")\n",
    "    if not weights:\n",
    "        print(f\"No trained model found for {ring_type}/{model_size}\")\n",
    "        return\n",
    "\n",
    "    run_dir = Path(weights).parent.parent\n",
    "    csv_path = run_dir / \"results.csv\"\n",
    "    if not csv_path.exists():\n",
    "        print(f\"results.csv not found at {csv_path}\")\n",
    "        return\n",
    "\n",
    "    epochs, box_loss, cls_loss, map50 = [], [], [], []\n",
    "    with open(csv_path) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            row = {k.strip(): v.strip() for k, v in row.items()}\n",
    "            epochs.append(int(row.get(\"epoch\", 0)))\n",
    "            box_loss.append(float(row.get(\"train/box_loss\", 0) or row.get(\"train/dfl_loss\", 0) or 0))\n",
    "            cls_loss.append(float(row.get(\"train/cls_loss\", 0) or 0))\n",
    "            map50.append(float(row.get(\"metrics/mAP50(B)\", 0) or 0))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    ax1.plot(epochs, box_loss, label=\"box loss\")\n",
    "    ax1.plot(epochs, cls_loss, label=\"cls loss\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(f\"{ring_type} / {model_size} - training loss\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2.plot(epochs, map50, color=\"green\", label=\"mAP50\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"mAP50\")\n",
    "    ax2.set_title(f\"{ring_type} / {model_size} - validation mAP50\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Change these to whatever ring type and model you want to inspect\n",
    "plot_training_curves(\"ice\", \"yolov8s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d1089",
   "metadata": {},
   "source": [
    "## 12. Test inference on new screenshots\n",
    "\n",
    "Test the trained model on screenshots that were NOT in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d6a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cores(image_path, ring_type, model_size=None, conf=CONF_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Run the trained detector on a single screenshot and display the result.\n",
    "\n",
    "    Args:\n",
    "        image_path: path to a .png or .jpg screenshot\n",
    "        ring_type: which ring type model to use (\"ice\", \"metallic\", \"rocky\", \"metal-rich\", \"unified\")\n",
    "        model_size: \"yolov8n\" etc, or None to auto-pick the best one\n",
    "        conf: confidence threshold\n",
    "    \"\"\"\n",
    "    results_path = RUNS_ROOT / \"all_results.json\"\n",
    "    if not results_path.exists():\n",
    "        print(\"No trained models found. Run training first.\")\n",
    "        return\n",
    "\n",
    "    with open(results_path) as f:\n",
    "        all_r = json.load(f)\n",
    "\n",
    "    ring_models = all_r.get(ring_type, {})\n",
    "    if not ring_models:\n",
    "        print(f\"No trained model for ring type '{ring_type}'.\")\n",
    "        return\n",
    "\n",
    "    if model_size is None:\n",
    "        model_size = max(ring_models, key=lambda m: ring_models[m][\"mAP50\"])\n",
    "        print(f\"Auto-selected best model: {model_size} (mAP50={ring_models[model_size]['mAP50']:.3f})\")\n",
    "\n",
    "    weights = ring_models.get(model_size, {}).get(\"weights\", \"\")\n",
    "    if not weights or not Path(weights).exists():\n",
    "        print(f\"Weights file not found: {weights}\")\n",
    "        return\n",
    "\n",
    "    model = YOLO(weights)\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        print(f\"Could not load image: {image_path}\")\n",
    "        return\n",
    "\n",
    "    preds = model.predict(img, conf=conf, verbose=False)[0]\n",
    "    annotated = preds.plot()\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.imshow(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"{ring_type} cores | {model_size} | {len(preds.boxes)} detection(s) | conf>={conf}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Detections: {len(preds.boxes)}\")\n",
    "    for box in preds.boxes:\n",
    "        xyxy = [round(v, 1) for v in box.xyxy[0].tolist()]\n",
    "        print(f\"  core | conf={float(box.conf):.3f} | bbox={xyxy}\")\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "# Usage - point at any screenshot and pick your ring type:\n",
    "# detect_cores(\"my_screenshot.png\", ring_type=\"ice\")\n",
    "# detect_cores(\"my_screenshot.png\", ring_type=\"metallic\", model_size=\"yolov8s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c7d463",
   "metadata": {},
   "source": [
    "## 13. One-button retrain\n",
    "\n",
    "After adding new labeled screenshots to the dataset folders, just call `retrain()`.\n",
    "It re-validates, regenerates configs, trains the best-performing model size for each\n",
    "ring type, and updates the results table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66fe86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain(ring_type=\"all\", model_size=\"best\"):\n",
    "    \"\"\"\n",
    "    Retrain after adding new labeled data. Drop new screenshots + labels into\n",
    "    the dataset folders, then call this.\n",
    "\n",
    "    Args:\n",
    "        ring_type: \"all\" retrains every ring type, or pass one e.g. \"ice\"\n",
    "        model_size: \"best\" reuses the model size that scored highest last time,\n",
    "                    or pass a specific size like \"yolov8s\"\n",
    "    \"\"\"\n",
    "    targets = RING_TYPES if ring_type == \"all\" else [ring_type]\n",
    "\n",
    "    results_path = RUNS_ROOT / \"all_results.json\"\n",
    "    prev_results = {}\n",
    "    if results_path.exists():\n",
    "        with open(results_path) as f:\n",
    "            prev_results = json.load(f)\n",
    "\n",
    "    for rt in targets:\n",
    "        stats, issues = validate_dataset(rt)\n",
    "        if issues:\n",
    "            print(f\"Dataset issues for '{rt}' - fix these before retraining:\")\n",
    "            for iss in issues:\n",
    "                print(f\"  {iss}\")\n",
    "            continue\n",
    "\n",
    "        total_images = stats.get(\"train\", {}).get(\"images\", 0)\n",
    "        if total_images < 5:\n",
    "            print(f\"Skipping '{rt}' - not enough images ({total_images}). Need at least 5.\")\n",
    "            continue\n",
    "\n",
    "        # pick model size\n",
    "        if model_size == \"best\":\n",
    "            ring_prev = prev_results.get(rt, {})\n",
    "            size = max(ring_prev, key=lambda m: ring_prev[m][\"mAP50\"]) if ring_prev else \"yolov8s\"\n",
    "            print(f\"Using best model from previous run: {size}\")\n",
    "        else:\n",
    "            size = model_size\n",
    "\n",
    "        generate_yaml(rt)\n",
    "        result = train_model(rt, size)\n",
    "\n",
    "        if result:\n",
    "            weights_path, train_results = result\n",
    "            rd = train_results.results_dict\n",
    "            if rt not in prev_results:\n",
    "                prev_results[rt] = {}\n",
    "            prev_results[rt][size] = {\n",
    "                \"weights\":   str(weights_path),\n",
    "                \"mAP50\":     float(rd.get(\"metrics/mAP50(B)\",     0)),\n",
    "                \"mAP50_95\":  float(rd.get(\"metrics/mAP50-95(B)\",  0)),\n",
    "                \"precision\": float(rd.get(\"metrics/precision(B)\", 0)),\n",
    "                \"recall\":    float(rd.get(\"metrics/recall(B)\",    0)),\n",
    "            }\n",
    "\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(prev_results, f, indent=2)\n",
    "\n",
    "    print(\"\\nRetrain complete!\")\n",
    "    show_comparison_table()\n",
    "\n",
    "\n",
    "# Examples:\n",
    "# retrain()                      # retrain all ring types using their best model\n",
    "# retrain(\"ice\")                 # retrain only the ice model\n",
    "# retrain(\"ice\", \"yolov8m\")      # force a specific model size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b62598",
   "metadata": {},
   "source": [
    "## 14. Export best models to ONNX\n",
    "\n",
    "ONNX lets the companion app run inference without needing PyTorch installed.\n",
    "It also opens the door to hardware acceleration via ONNX Runtime (which supports\n",
    "CUDA, DirectML on Windows, and plain CPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce78a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_best_models():\n",
    "    \"\"\"\n",
    "    Export the highest-scoring model for each ring type to ONNX format.\n",
    "    Output goes to exports/<ring_type>_best.onnx\n",
    "    \"\"\"\n",
    "    results_path = RUNS_ROOT / \"all_results.json\"\n",
    "    if not results_path.exists():\n",
    "        print(\"No trained models to export.\")\n",
    "        return\n",
    "\n",
    "    with open(results_path) as f:\n",
    "        all_r = json.load(f)\n",
    "\n",
    "    EXPORTS_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "    for ring_type, models in all_r.items():\n",
    "        if not models:\n",
    "            continue\n",
    "\n",
    "        best_size = max(models, key=lambda m: models[m][\"mAP50\"])\n",
    "        weights   = models[best_size][\"weights\"]\n",
    "\n",
    "        if not Path(weights).exists():\n",
    "            print(f\"Weights missing for {ring_type}/{best_size}: {weights}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Exporting {ring_type} ({best_size}, mAP50={models[best_size]['mAP50']:.3f})...\")\n",
    "        model = YOLO(weights)\n",
    "        exported = model.export(\n",
    "            format=\"onnx\",\n",
    "            imgsz=IMG_SIZE,\n",
    "            simplify=True,\n",
    "            opset=17,\n",
    "            dynamic=False,  # fixed input size, easier to deploy\n",
    "        )\n",
    "\n",
    "        dest = EXPORTS_ROOT / f\"{ring_type}_best.onnx\"\n",
    "        shutil.copy(exported, dest)\n",
    "        size_mb = dest.stat().st_size / 1024 / 1024\n",
    "        print(f\"  Saved: {dest}  ({size_mb:.1f} MB)\")\n",
    "\n",
    "    print(\"\\nExport complete. ONNX models are in:\", EXPORTS_ROOT)\n",
    "    print(\"Use these with onnxruntime in the companion app.\")\n",
    "\n",
    "\n",
    "export_best_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af8da0",
   "metadata": {},
   "source": [
    "## 15. Batch inference on a folder of screenshots\n",
    "\n",
    "Useful for testing on a full session's worth of screenshots at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90f184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_detect(screenshots_folder, ring_type, output_folder=None, conf=CONF_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Run detection on every image in a folder and save annotated copies.\n",
    "\n",
    "    Args:\n",
    "        screenshots_folder: folder containing .png/.jpg screenshots\n",
    "        ring_type: which ring type model to use\n",
    "        output_folder: where to save annotated images (defaults to screenshots_folder/detected)\n",
    "        conf: confidence threshold\n",
    "    \"\"\"\n",
    "    screenshots_folder = Path(screenshots_folder)\n",
    "    if output_folder is None:\n",
    "        output_folder = screenshots_folder / \"detected\"\n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    results_path = RUNS_ROOT / \"all_results.json\"\n",
    "    if not results_path.exists():\n",
    "        print(\"No trained models. Run training first.\")\n",
    "        return\n",
    "\n",
    "    with open(results_path) as f:\n",
    "        all_r = json.load(f)\n",
    "\n",
    "    ring_models = all_r.get(ring_type, {})\n",
    "    if not ring_models:\n",
    "        print(f\"No model for ring type '{ring_type}'\")\n",
    "        return\n",
    "\n",
    "    best_size = max(ring_models, key=lambda m: ring_models[m][\"mAP50\"])\n",
    "    weights   = ring_models[best_size][\"weights\"]\n",
    "    model     = YOLO(weights)\n",
    "\n",
    "    images = sorted(\n",
    "        list(screenshots_folder.glob(\"*.png\"))\n",
    "        + list(screenshots_folder.glob(\"*.jpg\"))\n",
    "    )\n",
    "    print(f\"Running {best_size} ({ring_type}) on {len(images)} images...\")\n",
    "\n",
    "    total_cores = 0\n",
    "    for img_path in images:\n",
    "        img   = cv2.imread(str(img_path))\n",
    "        preds = model.predict(img, conf=conf, verbose=False)[0]\n",
    "        annotated = preds.plot()\n",
    "        out_path = output_folder / img_path.name\n",
    "        cv2.imwrite(str(out_path), annotated)\n",
    "        total_cores += len(preds.boxes)\n",
    "\n",
    "    print(f\"Done. {total_cores} total core detections across {len(images)} screenshots.\")\n",
    "    print(f\"Annotated images saved to: {output_folder}\")\n",
    "\n",
    "\n",
    "# Usage:\n",
    "# batch_detect(\n",
    "#     screenshots_folder=r\"C:/Users/YourName/Pictures/Frontier Developments/Elite Dangerous\",\n",
    "#     ring_type=\"ice\",\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
