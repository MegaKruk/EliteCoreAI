{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Elite Dangerous Core Mining - Segmentation Detection Pipeline\n\nTrains YOLOv8 **instance segmentation** models to identify and outline core\nasteroids in Elite Dangerous screenshots in real time. Segmentation models draw\na polygon mask over each detected core rather than just a rectangle - a better\nfit for the irregular \"popcorn\" asteroid shapes.\n\n## Labeling in Roboflow\n\nUse the **Smart Select (SAM2/SAM3) polygon tool** exactly as you are doing.\nClick the asteroid and confirm the auto-traced outline. The SAM model used for\nannotation is Roboflow-internal and has nothing to do with your trained model.\n\nWhen exporting, choose: **YOLOv8 -> Segmentation** format.\nThe label files will contain polygon point coordinates instead of just a bbox.\n\n## Dataset structure\n\n```\ndatasets/\n  ice/\n    images/    <- all .png or .jpg screenshots, flat - no subfolders\n    labels/    <- matching YOLO segmentation .txt files\n  metallic/\n    images/\n    labels/\n  rocky/\n    images/\n    labels/\n  unified/     <- optional: all ring types combined\n    images/\n    labels/\n```\n\nYOLO segmentation label format (one line per core):\n`0 x1 y1 x2 y2 ... xn yn`\nClass index (0 = core) followed by normalized polygon point pairs (all 0.0-1.0).\nRoboflow exports this automatically when you pick the segmentation export format.\n\n## Models trained\n\nWe train and compare four YOLOv8-seg model sizes per ring type:\n- yolov8n-seg  fastest, smallest (~3.4M params)\n- yolov8s-seg  good balance (~11.8M params)\n- yolov8m-seg  higher accuracy (~27.3M params)\n- yolov8l-seg  best accuracy, slower inference (~46.5M params)\n\nAll four fit on RTX 3070 8GB at batch=16. K-fold cross-validation gives reliable\nmetric estimates without requiring manual train/val splits.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Install dependencies\n\nRun once. Restart the kernel after."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess, sys\n\npackages = [\n    \"ultralytics>=8.2.0\",\n    \"opencv-python>=4.9.0\",\n    \"pyyaml>=6.0.1\",\n    \"matplotlib>=3.9.0\",\n    \"scikit-learn>=1.5.0\",\n    \"pandas>=2.2.0\",\n    \"Pillow>=10.3.0\",\n    \"onnx>=1.16.0\",\n    \"onnxruntime-gpu>=1.18.0\",\n]\n\nsubprocess.run(\n    [sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\"] + packages,\n    check=True,\n)\nprint(\"All packages installed.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Imports and GPU check"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport json\nimport math\nimport shutil\nimport csv\nimport yaml\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom matplotlib.patches import Polygon as MplPolygon\nfrom matplotlib.collections import PatchCollection\n\nfrom pathlib import Path\nfrom datetime import datetime\nfrom collections import defaultdict\n\nimport torch\nfrom ultralytics import YOLO\nfrom sklearn.model_selection import KFold\n\nprint(f\"Python:  {sys.version}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    vram_gb = gpu.total_memory / 1024 ** 3\n    print(f\"GPU: {gpu.name}\")\n    print(f\"VRAM: {vram_gb:.1f} GB\")\n    if vram_gb < 6:\n        print(\"WARNING: less than 6 GB VRAM - reduce BATCH_SIZE to 8 if training crashes\")\nelse:\n    print(\"WARNING: no GPU found. Training on CPU will be very slow.\")\n    print(\"Reinstall PyTorch with CUDA support:\")\n    print(\"  pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---- MAIN CONFIG - edit this before running ----\n\n# Ring types to train. Remove any you don't have data for yet.\nRING_TYPES = [\"ice\", \"metallic\", \"rocky\", \"unified\"]\n\n# Segmentation model sizes to compare per ring type.\n# All four fit on RTX 3070 8GB at batch=16.\n# n=fastest, s=balanced, m=good accuracy, l=best accuracy\nMODEL_SIZES = [\"yolov8n-seg\", \"yolov8s-seg\", \"yolov8m-seg\", \"yolov8l-seg\"]\n\n# Number of cross-validation folds.\n# 5 is the default. Use 3 if you have fewer than ~30 images per ring type.\nK_FOLDS = 5\n\n# Training hyperparameters\nIMG_SIZE    = 640   # YOLO standard input size\nEPOCHS      = 100   # max epochs per fold (early stopping usually fires earlier)\nBATCH_SIZE  = 16    # reduce to 8 if you get CUDA out-of-memory errors\nPATIENCE    = 20    # stop early if val mAP doesn't improve for this many epochs\n\n# Confidence threshold for inference (0.0 - 1.0)\n# 0.4 is a good starting point. Tune based on real-game performance.\nCONF_THRESHOLD = 0.4\n\n# Mask overlay alpha for visualization (0=transparent, 1=opaque)\nMASK_ALPHA = 0.45\n\n# Core highlight color in BGR for OpenCV visualization\nMASK_COLOR_BGR = (0, 255, 128)   # bright green\n\n# We only detect one class.\nCLASS_NAMES = [\"core\"]\n\n# Folder layout\nDATASET_ROOT = Path(\"datasets\")\nRUNS_ROOT    = Path(\"runs\")\nEXPORTS_ROOT = Path(\"exports\")\nCV_TMP_ROOT  = Path(\"cv_tmp\")\n\nprint(\"Config loaded.\")\nprint(f\"Ring types: {RING_TYPES}\")\nprint(f\"Model sizes: {MODEL_SIZES}\")\nprint(f\"K-folds: {K_FOLDS}\")\ndevice_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\nprint(f\"Device: {device_name}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Create dataset folder structure"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_dataset_structure():\n    \"\"\"Create flat image and label folders for each ring type.\"\"\"\n    for ring_type in RING_TYPES:\n        (DATASET_ROOT / ring_type / \"images\").mkdir(parents=True, exist_ok=True)\n        (DATASET_ROOT / ring_type / \"labels\").mkdir(parents=True, exist_ok=True)\n\n    RUNS_ROOT.mkdir(exist_ok=True)\n    EXPORTS_ROOT.mkdir(exist_ok=True)\n    CV_TMP_ROOT.mkdir(exist_ok=True)\n\n    print(\"Folder structure created. Drop your files here:\\n\")\n    for ring_type in RING_TYPES:\n        print(f\"  datasets/{ring_type}/images/  <- all screenshots (.png or .jpg)\")\n        print(f\"  datasets/{ring_type}/labels/  <- YOLO segmentation .txt files from Roboflow\")\n        print()\n    print(\"No train/val split needed - k-fold handles it automatically.\")\n\n\ncreate_dataset_structure()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. COCO JSON to YOLO segmentation format converter\n\nOnly needed if you exported COCO JSON from Roboflow instead of YOLOv8 format.\nPrefer the direct YOLOv8 segmentation export from Roboflow and skip this.\n\nCOCO segmentation format stores polygon points in the `segmentation` field.\nYOLO segmentation format: `class x1 y1 x2 y2 ... xn yn` (normalized 0-1).\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def convert_coco_seg_to_yolo(coco_json_path, output_labels_dir):\n    \"\"\"\n    Convert a COCO-format segmentation JSON to per-image YOLO .txt label files.\n\n    COCO segmentation stores polygon points as flat pixel-coordinate lists in\n    the annotation's \"segmentation\" field, e.g. [x1,y1,x2,y2,...].\n    YOLO segmentation format: one line per object = class followed by normalized\n    polygon points: 0 x1 y1 x2 y2 ... xn yn  (all values 0.0-1.0).\n\n    If an annotation has no segmentation polygon but has a bbox, this function\n    falls back to converting the bbox into a 4-point rectangle polygon so the\n    annotation is not silently dropped.\n\n    Args:\n        coco_json_path: path to _annotations.coco.json\n        output_labels_dir: folder where .txt files will be written\n    \"\"\"\n    output_labels_dir = Path(output_labels_dir)\n    output_labels_dir.mkdir(parents=True, exist_ok=True)\n\n    with open(coco_json_path) as f:\n        coco = json.load(f)\n\n    cat_map = {}\n    for cat in coco[\"categories\"]:\n        if cat[\"name\"] in CLASS_NAMES:\n            cat_map[cat[\"id\"]] = CLASS_NAMES.index(cat[\"name\"])\n\n    if not cat_map:\n        print(f\"WARNING: no COCO categories match CLASS_NAMES {CLASS_NAMES}\")\n        print(f\"Categories in file: {[c['name'] for c in coco['categories']]}\")\n        return 0\n\n    images = {img[\"id\"]: img for img in coco[\"images\"]}\n\n    anns_by_image = defaultdict(list)\n    for ann in coco[\"annotations\"]:\n        if ann[\"category_id\"] in cat_map:\n            anns_by_image[ann[\"image_id\"]].append(ann)\n\n    converted = 0\n    fallback_bbox = 0\n\n    for img_id, anns in anns_by_image.items():\n        img_info = images[img_id]\n        W = img_info[\"width\"]\n        H = img_info[\"height\"]\n        img_name = Path(img_info[\"file_name\"]).stem\n        lines = []\n\n        for ann in anns:\n            cls = cat_map[ann[\"category_id\"]]\n            seg = ann.get(\"segmentation\", [])\n\n            if seg and isinstance(seg, list) and len(seg) > 0 and isinstance(seg[0], list):\n                # standard COCO polygon: list of rings, take the outer ring (first one)\n                points = seg[0]\n            elif seg and isinstance(seg, list) and len(seg) > 0 and isinstance(seg[0], (int, float)):\n                # flat list directly\n                points = seg\n            else:\n                # no polygon - fall back to bbox as a 4-corner rectangle\n                x, y, w, h = ann[\"bbox\"]\n                points = [x, y, x + w, y, x + w, y + h, x, y + h]\n                fallback_bbox += 1\n\n            if len(points) < 6:\n                # need at least 3 points (6 values) for a valid polygon\n                continue\n\n            # normalize all x,y pairs\n            norm = []\n            for i in range(0, len(points) - 1, 2):\n                norm.append(f\"{points[i] / W:.6f}\")\n                norm.append(f\"{points[i+1] / H:.6f}\")\n\n            lines.append(f\"{cls} \" + \" \".join(norm))\n\n        if lines:\n            (output_labels_dir / f\"{img_name}.txt\").write_text(\"\\n\".join(lines))\n            converted += 1\n\n    print(f\"Converted {converted} images -> {output_labels_dir}\")\n    if fallback_bbox > 0:\n        print(f\"  {fallback_bbox} annotation(s) had no polygon and were converted from bbox instead.\")\n        print(\"  Re-label those in Roboflow using the polygon tool for best results.\")\n    return converted\n\n\n# Example:\n# convert_coco_seg_to_yolo(\n#     coco_json_path=\"my_export/_annotations.coco.json\",\n#     output_labels_dir=\"datasets/ice/labels\",\n# )\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Validate datasets\n\nChecks that every image has a label, every label has valid segmentation polygon format, and the dataset is large enough for K_FOLDS."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def validate_dataset(ring_type):\n    \"\"\"\n    Check a ring type dataset for problems before training.\n    Returns (stats dict, list of issue strings). Empty issues = all clear.\n\n    Also checks that label files use segmentation format (10+ values per line)\n    rather than detection format (5 values). Warns if detection labels are found\n    so you know to re-export from Roboflow with the segmentation option.\n    \"\"\"\n    img_dir = DATASET_ROOT / ring_type / \"images\"\n    lbl_dir = DATASET_ROOT / ring_type / \"labels\"\n\n    images = list(img_dir.glob(\"*.png\")) + list(img_dir.glob(\"*.jpg\"))\n    labels = list(lbl_dir.glob(\"*.txt\"))\n\n    img_stems = {p.stem for p in images}\n    lbl_stems = {p.stem for p in labels}\n\n    issues = []\n\n    missing = img_stems - lbl_stems\n    orphans = lbl_stems - img_stems\n    if missing:\n        issues.append(f\"{len(missing)} image(s) have no label: {sorted(missing)[:5]}\")\n    if orphans:\n        issues.append(f\"{len(orphans)} label(s) have no image: {sorted(orphans)[:5]}\")\n\n    total_cores    = 0\n    bad_lines      = 0\n    detection_lbls = 0   # lines that look like detection format (5 values only)\n\n    for lbl in labels:\n        for line in lbl.read_text().strip().splitlines():\n            line = line.strip()\n            if not line:\n                continue\n            parts = line.split()\n            if len(parts) < 7:\n                # less than 3 polygon points (class + 3*2=6 values minimum)\n                if len(parts) == 5:\n                    detection_lbls += 1\n                else:\n                    bad_lines += 1\n                continue\n            try:\n                cls = int(parts[0])\n                coords = [float(v) for v in parts[1:]]\n                if any(v < 0 or v > 1 for v in coords):\n                    bad_lines += 1\n                elif cls == 0:\n                    total_cores += 1\n            except ValueError:\n                bad_lines += 1\n\n    if bad_lines:\n        issues.append(f\"{bad_lines} malformed label line(s)\")\n    if detection_lbls > 0:\n        issues.append(\n            f\"{detection_lbls} label line(s) look like detection format (5 values). \"\n            \"Re-export from Roboflow using YOLOv8 Segmentation format.\"\n        )\n\n    paired     = len(img_stems & lbl_stems)\n    min_needed = K_FOLDS * 2\n    if paired < min_needed:\n        issues.append(\n            f\"only {paired} labeled images but K_FOLDS={K_FOLDS} needs >= {min_needed}. \"\n            \"Add more data or reduce K_FOLDS in config.\"\n        )\n\n    stats = {\n        \"images\":  len(images),\n        \"paired\":  paired,\n        \"cores\":   total_cores,\n        \"avg_cores_per_image\": round(total_cores / paired, 2) if paired else 0,\n    }\n\n    print(f\"\\n{'=' * 44}\")\n    print(f\"  {ring_type.upper()} dataset\")\n    print(f\"{'=' * 44}\")\n    print(f\"  Images:              {stats['images']}\")\n    print(f\"  Paired (img+label):  {stats['paired']}\")\n    print(f\"  Total core polygons: {stats['cores']}\")\n    print(f\"  Avg cores/image:     {stats['avg_cores_per_image']}\")\n\n    if issues:\n        print(\"  ISSUES:\")\n        for iss in issues:\n            print(f\"    - {iss}\")\n    else:\n        print(\"  All checks passed.\")\n\n    return stats, issues\n\n\nfor ring_type in RING_TYPES:\n    validate_dataset(ring_type)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Preview labeled images\n\nDraws the segmentation polygons from your label files over the screenshots. Use this to confirm Roboflow exported correctly before wasting time training on bad labels."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def preview_labels(ring_type, n=4):\n    \"\"\"\n    Show the first n labeled images with segmentation polygons drawn on them.\n    Each core gets a filled semi-transparent polygon plus its outline.\n    \"\"\"\n    img_dir = DATASET_ROOT / ring_type / \"images\"\n    lbl_dir = DATASET_ROOT / ring_type / \"labels\"\n\n    images = sorted(list(img_dir.glob(\"*.png\")) + list(img_dir.glob(\"*.jpg\")))[:n]\n    if not images:\n        print(f\"No images found in {img_dir}\")\n        return\n\n    cols = min(len(images), 4)\n    rows = math.ceil(len(images) / cols)\n    fig, axes = plt.subplots(rows, cols, figsize=(6 * cols, 5 * rows))\n\n    if rows == 1 and cols == 1:\n        axes = [axes]\n    elif rows == 1 or cols == 1:\n        axes = list(axes.flat)\n    else:\n        axes = [ax for row in axes for ax in row]\n\n    for i, ax in enumerate(axes):\n        if i >= len(images):\n            ax.axis(\"off\")\n            continue\n\n        img_path = images[i]\n        img = cv2.imread(str(img_path))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        H, W = img.shape[:2]\n\n        ax.imshow(img)\n\n        lbl_path = lbl_dir / (img_path.stem + \".txt\")\n        if lbl_path.exists():\n            patches = []\n            for line in lbl_path.read_text().strip().splitlines():\n                parts = line.strip().split()\n                if len(parts) < 7:\n                    continue\n                coords = [float(v) for v in parts[1:]]\n                # convert normalized pairs to pixel coordinates\n                pts = np.array([\n                    [coords[j] * W, coords[j + 1] * H]\n                    for j in range(0, len(coords) - 1, 2)\n                ])\n                poly = MplPolygon(pts, closed=True)\n                patches.append(poly)\n                # draw outline\n                ax.plot(\n                    np.append(pts[:, 0], pts[0, 0]),\n                    np.append(pts[:, 1], pts[0, 1]),\n                    color=\"lime\", linewidth=1.5\n                )\n                # label at centroid\n                cx, cy = pts[:, 0].mean(), pts[:, 1].mean()\n                ax.text(cx, cy, \"core\", color=\"white\", fontsize=7,\n                        ha=\"center\", va=\"center\",\n                        bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"green\", alpha=0.6))\n\n            if patches:\n                pc = PatchCollection(patches, alpha=0.25, facecolor=\"lime\", edgecolor=\"none\")\n                ax.add_collection(pc)\n\n        ax.set_title(img_path.name, fontsize=8)\n        ax.axis(\"off\")\n\n    plt.suptitle(f\"{ring_type} - segmentation label preview\", fontsize=12)\n    plt.tight_layout()\n    plt.show()\n\n\npreview_labels(\"ice\", n=4)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. K-fold cross-validation helpers\n\nSame approach as before: build temporary per-fold datasets using hard links,\nwrite a data.yaml per fold, train, then clean up. No extra disk space used.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_paired_samples(ring_type):\n    \"\"\"Return sorted list of (image_path, label_path) pairs that both exist.\"\"\"\n    img_dir = DATASET_ROOT / ring_type / \"images\"\n    lbl_dir = DATASET_ROOT / ring_type / \"labels\"\n    pairs = []\n    for img_path in sorted(list(img_dir.glob(\"*.png\")) + list(img_dir.glob(\"*.jpg\"))):\n        lbl_path = lbl_dir / (img_path.stem + \".txt\")\n        if lbl_path.exists():\n            pairs.append((img_path, lbl_path))\n    return pairs\n\n\ndef build_fold_dataset(pairs_train, pairs_val, fold_dir):\n    \"\"\"\n    Build a temporary YOLO dataset for one fold using hard links.\n    Falls back to file copy if the filesystem doesn't support hard links.\n    \"\"\"\n    def link_or_copy(src, dst):\n        dst.parent.mkdir(parents=True, exist_ok=True)\n        if dst.exists():\n            dst.unlink()\n        try:\n            os.link(src, dst)\n        except OSError:\n            shutil.copy2(src, dst)\n\n    for split, pairs in [(\"train\", pairs_train), (\"val\", pairs_val)]:\n        for img_path, lbl_path in pairs:\n            link_or_copy(img_path, fold_dir / \"images\" / split / img_path.name)\n            link_or_copy(lbl_path, fold_dir / \"labels\" / split / lbl_path.name)\n\n\ndef write_fold_yaml(fold_dir):\n    \"\"\"Write data.yaml pointing YOLO at this fold's train/val folders.\"\"\"\n    yaml_path = fold_dir / \"data.yaml\"\n    with open(yaml_path, \"w\") as f:\n        yaml.dump({\n            \"path\":  str(fold_dir.resolve()),\n            \"train\": \"images/train\",\n            \"val\":   \"images/val\",\n            \"nc\":    len(CLASS_NAMES),\n            \"names\": CLASS_NAMES,\n        }, f, default_flow_style=False)\n    return yaml_path\n\n\ndef cleanup_fold(fold_dir):\n    \"\"\"Delete temporary fold dataset after training is done.\"\"\"\n    if fold_dir.exists():\n        shutil.rmtree(fold_dir)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_one_fold(yaml_path, run_name, model_size, n_train, n_val):\n    \"\"\"\n    Train one YOLOv8-seg model on a prepared fold dataset.\n\n    Segmentation models output both a bounding box head and a mask head.\n    We track both box-mAP (mAP50(B)) and mask-mAP (mAP50(M)) in results.\n\n    Returns:\n        (weights_path str, metrics dict)\n    \"\"\"\n    print(f\"  Training {run_name}  (train={n_train}, val={n_val})\")\n\n    model = YOLO(f\"{model_size}.pt\")\n\n    results = model.train(\n        data=str(yaml_path),\n        epochs=EPOCHS,\n        imgsz=IMG_SIZE,\n        batch=BATCH_SIZE,\n        patience=PATIENCE,\n        device=0 if torch.cuda.is_available() else \"cpu\",\n        project=str(RUNS_ROOT),\n        name=run_name,\n\n        # -- Augmentation tuned for Elite Dangerous screenshots --\n        # Asteroids rotate at all angles - heavy rotation is always valid\n        degrees=45,\n        # Horizontal/vertical flips are valid in zero-g environments\n        fliplr=0.5,\n        flipud=0.3,\n        # Cores appear at different distances = different apparent sizes on screen\n        scale=0.5,\n        # Mosaic helps the model handle cluttered asteroid fields\n        mosaic=1.0,\n        close_mosaic=10,\n        # Brightness and saturation shift for star-lit vs shadow-side-of-planet lighting\n        hsv_v=0.4,\n        hsv_s=0.5,\n        # Small hue shift - ring types differ somewhat but not wildly\n        hsv_h=0.015,\n        # Game renders are sharp - skip blur augmentations\n        blur=False,\n        median_blur=0.0,\n\n        save_period=0,   # skip intermediate checkpoints to save disk space\n        verbose=False,\n    )\n\n    rd = results.results_dict\n    weights = RUNS_ROOT / run_name / \"weights\" / \"best.pt\"\n\n    metrics = {\n        # box metrics (bounding box head)\n        \"box_mAP50\":    float(rd.get(\"metrics/mAP50(B)\",     0)),\n        \"box_mAP50_95\": float(rd.get(\"metrics/mAP50-95(B)\",  0)),\n        \"precision\":    float(rd.get(\"metrics/precision(B)\", 0)),\n        \"recall\":       float(rd.get(\"metrics/recall(B)\",    0)),\n        # mask metrics (segmentation head) - the primary metric for seg models\n        \"mask_mAP50\":    float(rd.get(\"metrics/mAP50(M)\",    0)),\n        \"mask_mAP50_95\": float(rd.get(\"metrics/mAP50-95(M)\", 0)),\n    }\n\n    print(\n        f\"    -> box mAP50={metrics['box_mAP50']:.3f}  \"\n        f\"mask mAP50={metrics['mask_mAP50']:.3f}  \"\n        f\"P={metrics['precision']:.3f}  R={metrics['recall']:.3f}\"\n    )\n    return str(weights), metrics\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Train segmentation models with k-fold cross-validation\n\nFor each ring type x model size combination:\n1. Split all images into K folds\n2. Train K models (each validated on a different held-out chunk)\n3. Average both box-mAP and mask-mAP across folds\n4. Keep the best fold's weights as the deployed model\n\n**Mask mAP50** is the primary metric for segmentation - it measures how well the\npredicted polygon overlaps with your labeled polygon. Box mAP50 is the secondary\nmetric from the detection head (still useful for knowing if the model finds the\nasteroid at all, even if the mask shape isn't perfect).\n\nRough timing on RTX 3070:\n- yolov8n-seg: ~5 min/fold\n- yolov8s-seg: ~10 min/fold\n- yolov8m-seg: ~20 min/fold\n- yolov8l-seg: ~35 min/fold\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_kfold(ring_type, model_size):\n    \"\"\"\n    Run K-fold cross-validation for one ring type + model size combination.\n    Best fold is selected by mask_mAP50 (the primary seg metric).\n    Returns a result dict or None if not enough data.\n    \"\"\"\n    pairs = get_paired_samples(ring_type)\n\n    if len(pairs) < K_FOLDS * 2:\n        print(f\"Skipping {ring_type}/{model_size}: only {len(pairs)} paired samples \"\n              f\"(need >= {K_FOLDS * 2} for {K_FOLDS} folds).\")\n        return None\n\n    print(f\"\\n{'=' * 56}\")\n    print(f\"  {ring_type.upper()} / {model_size}  ({len(pairs)} images, {K_FOLDS} folds)\")\n    print(f\"{'=' * 56}\")\n\n    kf        = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n    pairs_arr = np.array(pairs, dtype=object)\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    fold_results = []\n\n    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(pairs_arr)):\n        fold_num    = fold_idx + 1\n        pairs_train = pairs_arr[train_idx].tolist()\n        pairs_val   = pairs_arr[val_idx].tolist()\n\n        fold_dir = CV_TMP_ROOT / f\"{ring_type}_{model_size}_fold{fold_num}\"\n        run_name = f\"{ring_type}_{model_size}_fold{fold_num}_{timestamp}\"\n\n        print(f\"\\nFold {fold_num}/{K_FOLDS}\")\n        build_fold_dataset(pairs_train, pairs_val, fold_dir)\n        yaml_path = write_fold_yaml(fold_dir)\n\n        weights, metrics = train_one_fold(\n            yaml_path, run_name, model_size, len(pairs_train), len(pairs_val)\n        )\n        fold_results.append({\"fold\": fold_num, \"weights\": weights, **metrics})\n        cleanup_fold(fold_dir)\n\n    def avg(key):\n        return float(np.mean([r[key] for r in fold_results]))\n\n    def std(key):\n        return float(np.std([r[key] for r in fold_results]))\n\n    # best fold selected by mask mAP50 (primary seg metric)\n    best_fold = max(fold_results, key=lambda r: r[\"mask_mAP50\"])\n\n    summary = {\n        \"best_weights\":    best_fold[\"weights\"],\n        \"best_fold\":       best_fold[\"fold\"],\n        \"fold_results\":    fold_results,\n        \"box_mAP50\":       avg(\"box_mAP50\"),\n        \"box_mAP50_95\":    avg(\"box_mAP50_95\"),\n        \"mask_mAP50\":      avg(\"mask_mAP50\"),\n        \"mask_mAP50_95\":   avg(\"mask_mAP50_95\"),\n        \"precision\":       avg(\"precision\"),\n        \"recall\":          avg(\"recall\"),\n        \"mask_mAP50_std\":  std(\"mask_mAP50\"),\n        \"box_mAP50_std\":   std(\"box_mAP50\"),\n    }\n\n    print(f\"\\nAverage across {K_FOLDS} folds:\")\n    print(\n        f\"  box  mAP50={summary['box_mAP50']:.3f} (+/-{summary['box_mAP50_std']:.3f})\"\n    )\n    print(\n        f\"  mask mAP50={summary['mask_mAP50']:.3f} (+/-{summary['mask_mAP50_std']:.3f})  \"\n        f\"<-- primary metric\"\n    )\n    print(\n        f\"  P={summary['precision']:.3f}  R={summary['recall']:.3f}  \"\n        f\"best fold: {best_fold['fold']}\"\n    )\n    return summary\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run all ring type x model size combinations.\n# Skips any ring type that doesn't have enough data yet.\n\nall_results = {}\n\nfor ring_type in RING_TYPES:\n    stats, issues = validate_dataset(ring_type)\n    if issues:\n        print(f\"Skipping {ring_type} - fix dataset issues first:\")\n        for iss in issues:\n            print(f\"  {iss}\")\n        continue\n    if stats[\"paired\"] < K_FOLDS * 2:\n        print(f\"Skipping {ring_type} - not enough images ({stats['paired']}).\")\n        continue\n\n    all_results[ring_type] = {}\n    for model_size in MODEL_SIZES:\n        result = train_kfold(ring_type, model_size)\n        if result is not None:\n            all_results[ring_type][model_size] = result\n\nresults_path = RUNS_ROOT / \"all_results.json\"\nRUNS_ROOT.mkdir(exist_ok=True)\nwith open(results_path, \"w\") as f:\n    json.dump(all_results, f, indent=2)\n\nprint(\"\\nAll training runs complete. Results saved to:\", results_path)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Compare model results\n\nBoth box mAP50 and mask mAP50 are shown. Mask mAP50 is what matters for segmentation quality - it measures polygon overlap with your ground truth labels."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def show_comparison_table():\n    \"\"\"\n    Print a formatted table of all trained model metrics.\n    Shows both box and mask mAP50 with standard deviation across folds.\n    Best model per ring type is selected by mask_mAP50.\n    \"\"\"\n    results_path = RUNS_ROOT / \"all_results.json\"\n    if not results_path.exists():\n        print(\"No results yet - run training first.\")\n        return {}\n\n    with open(results_path) as f:\n        results = json.load(f)\n\n    header = (\n        f\"{'Ring':<12} {'Model':<16} \"\n        f\"{'boxmAP50':>9} {'  +/-':>6} \"\n        f\"{'mskMAP50':>9} {'  +/-':>6} \"\n        f\"{'Prec':>7} {'Recall':>7}\"\n    )\n    print(header)\n    print(\"-\" * len(header))\n\n    best_per_ring = {}\n    for ring_type, models in results.items():\n        best_mask = -1\n        for model_size, m in models.items():\n            marker = \"\"\n            if m[\"mask_mAP50\"] > best_mask:\n                best_mask = m[\"mask_mAP50\"]\n                best_per_ring[ring_type] = model_size\n                marker = \"  <-- best\"\n            print(\n                f\"{ring_type:<12} {model_size:<16} \"\n                f\"{m['box_mAP50']:>9.3f} {m.get('box_mAP50_std', 0):>6.3f} \"\n                f\"{m['mask_mAP50']:>9.3f} {m.get('mask_mAP50_std', 0):>6.3f} \"\n                f\"{m['precision']:>7.3f} {m['recall']:>7.3f}\"\n                + marker\n            )\n        print()\n\n    return best_per_ring\n\n\nbest_models = show_comparison_table()\nprint(\"Best model per ring type:\", best_models)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. Plot per-fold metrics\n\nHigh variance between folds = model is sensitive to which images land in val, which usually means you need more labeled data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_fold_metrics(ring_type, model_size):\n    \"\"\"\n    Bar chart of box mAP50 and mask mAP50 per fold side by side.\n    Dashed lines show the mean across folds for each metric.\n    \"\"\"\n    results_path = RUNS_ROOT / \"all_results.json\"\n    if not results_path.exists():\n        print(\"No results found.\")\n        return\n\n    with open(results_path) as f:\n        all_r = json.load(f)\n\n    model_data = all_r.get(ring_type, {}).get(model_size)\n    if not model_data:\n        print(f\"No data for {ring_type}/{model_size}\")\n        return\n\n    fold_results = model_data.get(\"fold_results\", [])\n    if not fold_results:\n        print(\"No per-fold results stored.\")\n        return\n\n    folds      = [f\"Fold {r['fold']}\" for r in fold_results]\n    box_maps   = [r[\"box_mAP50\"]  for r in fold_results]\n    mask_maps  = [r[\"mask_mAP50\"] for r in fold_results]\n    precs      = [r[\"precision\"]  for r in fold_results]\n    recs       = [r[\"recall\"]     for r in fold_results]\n\n    x = np.arange(len(folds))\n    w = 0.2\n\n    fig, ax = plt.subplots(figsize=(11, 5))\n    ax.bar(x - 1.5*w, box_maps,  w, label=\"box mAP50\",  color=\"steelblue\")\n    ax.bar(x - 0.5*w, mask_maps, w, label=\"mask mAP50\", color=\"darkorange\")\n    ax.bar(x + 0.5*w, precs,     w, label=\"Precision\",  color=\"seagreen\")\n    ax.bar(x + 1.5*w, recs,      w, label=\"Recall\",     color=\"tomato\")\n\n    ax.axhline(np.mean(box_maps),  color=\"steelblue\",  linestyle=\"--\", linewidth=1.1,\n               label=f\"mean box={np.mean(box_maps):.3f}\")\n    ax.axhline(np.mean(mask_maps), color=\"darkorange\",  linestyle=\"--\", linewidth=1.1,\n               label=f\"mean mask={np.mean(mask_maps):.3f}\")\n\n    ax.set_xticks(x)\n    ax.set_xticklabels(folds)\n    ax.set_ylim(0, 1.05)\n    ax.set_ylabel(\"Score\")\n    ax.set_title(f\"{ring_type} / {model_size} - metrics per fold\")\n    ax.legend(fontsize=8)\n    ax.grid(True, axis=\"y\", alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n\nplot_fold_metrics(\"ice\", \"yolov8s-seg\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 12. Plot training curves for a specific fold"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_training_curves(ring_type, model_size, fold=None):\n    \"\"\"\n    Plot loss, box mAP50, and mask mAP50 curves from a run's results.csv.\n    Defaults to the best fold if fold is not specified.\n    \"\"\"\n    results_path = RUNS_ROOT / \"all_results.json\"\n    if not results_path.exists():\n        print(\"No results found.\")\n        return\n\n    with open(results_path) as f:\n        all_r = json.load(f)\n\n    model_data = all_r.get(ring_type, {}).get(model_size)\n    if not model_data:\n        print(f\"No data for {ring_type}/{model_size}\")\n        return\n\n    if fold is None:\n        fold = model_data[\"best_fold\"]\n        print(f\"Plotting best fold: {fold}\")\n\n    weights_path = next(\n        (r[\"weights\"] for r in model_data[\"fold_results\"] if r[\"fold\"] == fold), None\n    )\n    if not weights_path:\n        print(f\"Fold {fold} not found.\")\n        return\n\n    csv_path = Path(weights_path).parent.parent / \"results.csv\"\n    if not csv_path.exists():\n        print(f\"results.csv not found at {csv_path}\")\n        return\n\n    epochs, seg_loss, box_loss, cls_loss, box_map50, mask_map50 = [], [], [], [], [], []\n    with open(csv_path) as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            row = {k.strip(): v.strip() for k, v in row.items()}\n            epochs.append(int(row.get(\"epoch\", 0)))\n            # seg loss key varies slightly by ultralytics version\n            seg_loss.append(float(\n                row.get(\"train/seg_loss\") or row.get(\"train/dfl_loss\") or 0\n            ))\n            box_loss.append(float(row.get(\"train/box_loss\") or 0))\n            cls_loss.append(float(row.get(\"train/cls_loss\") or 0))\n            box_map50.append(float(row.get(\"metrics/mAP50(B)\") or 0))\n            mask_map50.append(float(row.get(\"metrics/mAP50(M)\") or 0))\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 4))\n\n    ax1.plot(epochs, seg_loss,  label=\"seg loss\")\n    ax1.plot(epochs, box_loss,  label=\"box loss\")\n    ax1.plot(epochs, cls_loss,  label=\"cls loss\")\n    ax1.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.set_title(f\"{ring_type} / {model_size} fold {fold} - training losses\")\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    ax2.plot(epochs, box_map50,  label=\"box mAP50\",  color=\"steelblue\")\n    ax2.plot(epochs, mask_map50, label=\"mask mAP50\", color=\"darkorange\", linewidth=2)\n    ax2.set_xlabel(\"Epoch\")\n    ax2.set_ylabel(\"mAP50\")\n    ax2.set_title(f\"{ring_type} / {model_size} fold {fold} - validation mAP50\")\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n\nplot_training_curves(\"ice\", \"yolov8s-seg\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 13. Test inference on a screenshot\n\nRuns the segmentation model and draws filled polygon masks over detected cores with confidence scores."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def detect_cores(image_path, ring_type, model_size=None, conf=CONF_THRESHOLD):\n    \"\"\"\n    Run the trained segmentation model on a single screenshot.\n    Draws filled polygon masks over each detected core.\n\n    Args:\n        image_path: path to a .png or .jpg screenshot\n        ring_type:  which ring type model to load\n        model_size: e.g. \"yolov8s-seg\", or None to auto-pick best mask mAP50\n        conf:       confidence threshold\n    \"\"\"\n    results_path = RUNS_ROOT / \"all_results.json\"\n    if not results_path.exists():\n        print(\"No trained models found. Run training first.\")\n        return\n\n    with open(results_path) as f:\n        all_r = json.load(f)\n\n    ring_models = all_r.get(ring_type, {})\n    if not ring_models:\n        print(f\"No trained model for ring type '{ring_type}'.\")\n        return\n\n    if model_size is None:\n        model_size = max(ring_models, key=lambda m: ring_models[m][\"mask_mAP50\"])\n        m = ring_models[model_size]\n        print(\n            f\"Auto-selected: {model_size} \"\n            f\"(mask mAP50={m['mask_mAP50']:.3f}, box mAP50={m['box_mAP50']:.3f})\"\n        )\n\n    weights = ring_models.get(model_size, {}).get(\"best_weights\", \"\")\n    if not weights or not Path(weights).exists():\n        print(f\"Weights file not found: {weights}\")\n        return\n\n    model = YOLO(weights)\n    img   = cv2.imread(str(image_path))\n    if img is None:\n        print(f\"Could not load image: {image_path}\")\n        return\n\n    preds = model.predict(img, conf=conf, verbose=False)[0]\n\n    # draw masks manually so we control colors and alpha\n    img_rgb   = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).copy()\n    overlay   = img_rgb.copy()\n    H, W      = img_rgb.shape[:2]\n\n    if preds.masks is not None:\n        for i, mask_data in enumerate(preds.masks.xy):\n            pts = mask_data.astype(np.int32)\n            cv2.fillPoly(overlay, [pts], color=MASK_COLOR_BGR[::-1])  # BGR->RGB\n\n        cv2.addWeighted(overlay, MASK_ALPHA, img_rgb, 1 - MASK_ALPHA, 0, img_rgb)\n\n        # draw polygon outlines and confidence labels on top\n        for i, (mask_data, box) in enumerate(zip(preds.masks.xy, preds.boxes)):\n            pts  = mask_data.astype(np.int32)\n            conf_val = float(box.conf)\n            cv2.polylines(img_rgb, [pts], isClosed=True, color=(0, 255, 80), thickness=2)\n            cx = int(pts[:, 0].mean())\n            cy = int(pts[:, 1].mean())\n            label = f\"core {conf_val:.2f}\"\n            cv2.putText(img_rgb, label, (cx - 30, cy),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n\n    plt.figure(figsize=(14, 8))\n    plt.imshow(img_rgb)\n    n = len(preds.boxes)\n    plt.title(f\"{ring_type} | {model_size} | {n} core(s) detected | conf>={conf}\")\n    plt.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\n    print(f\"Detections: {n}\")\n    for box in preds.boxes:\n        xyxy = [round(v, 1) for v in box.xyxy[0].tolist()]\n        print(f\"  core | conf={float(box.conf):.3f} | bbox={xyxy}\")\n\n    return preds\n\n\n# Usage:\n# detect_cores(\"screenshot.png\", ring_type=\"ice\")\n# detect_cores(\"screenshot.png\", ring_type=\"metallic\", model_size=\"yolov8m-seg\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 14. One-button retrain\n\nDrop new labeled screenshots into the dataset folder and call `retrain()`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def retrain(ring_type=\"all\", model_size=\"best\"):\n    \"\"\"\n    Retrain after adding new labeled screenshots.\n    Drop new images + label files into datasets/<ring_type>/images/ and labels/,\n    then call this.\n\n    Args:\n        ring_type:  \"all\" retrains every type, or pass one e.g. \"ice\"\n        model_size: \"best\" reuses the size that scored highest last time by\n                    mask_mAP50, or pass a specific size like \"yolov8m-seg\"\n    \"\"\"\n    targets = RING_TYPES if ring_type == \"all\" else [ring_type]\n\n    results_path = RUNS_ROOT / \"all_results.json\"\n    prev_results = {}\n    if results_path.exists():\n        with open(results_path) as f:\n            prev_results = json.load(f)\n\n    for rt in targets:\n        stats, issues = validate_dataset(rt)\n        if issues:\n            print(f\"Dataset issues for '{rt}':\")\n            for iss in issues:\n                print(f\"  {iss}\")\n            continue\n\n        if stats[\"paired\"] < K_FOLDS * 2:\n            print(f\"Skipping '{rt}' - not enough images ({stats['paired']}). \"\n                  f\"Need >= {K_FOLDS * 2}.\")\n            continue\n\n        if model_size == \"best\":\n            ring_prev = prev_results.get(rt, {})\n            size = (\n                max(ring_prev, key=lambda m: ring_prev[m][\"mask_mAP50\"])\n                if ring_prev else \"yolov8s-seg\"\n            )\n            print(f\"Using best model size from previous run: {size}\")\n        else:\n            size = model_size\n\n        result = train_kfold(rt, size)\n        if result is not None:\n            if rt not in prev_results:\n                prev_results[rt] = {}\n            prev_results[rt][size] = result\n\n    with open(results_path, \"w\") as f:\n        json.dump(prev_results, f, indent=2)\n\n    print(\"\\nRetrain complete!\")\n    show_comparison_table()\n\n\n# Examples:\n# retrain()                          # all ring types, best model size each\n# retrain(\"ice\")                     # only ice, auto-pick best size\n# retrain(\"ice\", \"yolov8l-seg\")      # force a specific model size\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 15. Export best models to ONNX"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def export_best_models():\n    \"\"\"\n    Export the best fold's weights for each ring type to ONNX format.\n    Best is chosen by mask_mAP50. Output goes to exports/<ring_type>_best.onnx\n    \"\"\"\n    results_path = RUNS_ROOT / \"all_results.json\"\n    if not results_path.exists():\n        print(\"No trained models to export.\")\n        return\n\n    with open(results_path) as f:\n        all_r = json.load(f)\n\n    EXPORTS_ROOT.mkdir(exist_ok=True)\n\n    for ring_type, models in all_r.items():\n        if not models:\n            continue\n\n        best_size    = max(models, key=lambda m: models[m][\"mask_mAP50\"])\n        best_weights = models[best_size][\"best_weights\"]\n\n        if not Path(best_weights).exists():\n            print(f\"Weights missing for {ring_type}/{best_size}: {best_weights}\")\n            continue\n\n        m = models[best_size]\n        print(\n            f\"Exporting {ring_type} ({best_size}) \"\n            f\"mask mAP50={m['mask_mAP50']:.3f} box mAP50={m['box_mAP50']:.3f}...\"\n        )\n        model    = YOLO(best_weights)\n        exported = model.export(\n            format=\"onnx\",\n            imgsz=IMG_SIZE,\n            simplify=True,\n            opset=17,\n            dynamic=False,\n        )\n\n        dest    = EXPORTS_ROOT / f\"{ring_type}_best.onnx\"\n        shutil.copy(exported, dest)\n        print(f\"  -> {dest}  ({dest.stat().st_size / 1024 / 1024:.1f} MB)\")\n\n    print(\"\\nExport complete. Files are in:\", EXPORTS_ROOT)\n\n\nexport_best_models()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 16. Batch inference on a folder of screenshots"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def batch_detect(screenshots_folder, ring_type, output_folder=None, conf=CONF_THRESHOLD):\n    \"\"\"\n    Run segmentation detection on every image in a folder and save annotated copies.\n    Annotated copies have filled polygon masks drawn over detected cores.\n\n    Args:\n        screenshots_folder: folder with .png/.jpg screenshots\n        ring_type:          which ring type model to use\n        output_folder:      where to save annotated images (default: screenshots_folder/detected)\n        conf:               confidence threshold\n    \"\"\"\n    screenshots_folder = Path(screenshots_folder)\n    output_folder = Path(output_folder or screenshots_folder / \"detected\")\n    output_folder.mkdir(exist_ok=True)\n\n    results_path = RUNS_ROOT / \"all_results.json\"\n    if not results_path.exists():\n        print(\"No trained models. Run training first.\")\n        return\n\n    with open(results_path) as f:\n        all_r = json.load(f)\n\n    ring_models = all_r.get(ring_type, {})\n    if not ring_models:\n        print(f\"No model for ring type '{ring_type}'\")\n        return\n\n    best_size    = max(ring_models, key=lambda m: ring_models[m][\"mask_mAP50\"])\n    best_weights = ring_models[best_size][\"best_weights\"]\n    model        = YOLO(best_weights)\n\n    images = sorted(\n        list(screenshots_folder.glob(\"*.png\"))\n        + list(screenshots_folder.glob(\"*.jpg\"))\n    )\n    print(f\"Running {best_size} ({ring_type}) on {len(images)} images...\")\n\n    total_cores = 0\n    for img_path in images:\n        img   = cv2.imread(str(img_path))\n        preds = model.predict(img, conf=conf, verbose=False)[0]\n\n        # preds.plot() renders masks + boxes + labels into one annotated image\n        annotated = preds.plot()\n        cv2.imwrite(str(output_folder / img_path.name), annotated)\n        total_cores += len(preds.boxes)\n\n    print(f\"Done. {total_cores} total core detections across {len(images)} screenshots.\")\n    print(f\"Annotated images saved to: {output_folder}\")\n\n\n# Usage:\n# batch_detect(\n#     screenshots_folder=r\"C:/Users/YourName/Pictures/Frontier Developments/Elite Dangerous\",\n#     ring_type=\"ice\",\n# )\n"
  }
 ]
}