{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elite Dangerous Core Mining - ML Detection Pipeline\n\nTrains YOLOv8 object-detection models to identify core asteroids in Elite\nDangerous screenshots in real time.\n\n## Labeling tool recommendation\n\nYou need to draw bounding boxes around every core asteroid visible in each\nscreenshot before training. Best free options:\n\n**Recommended: [Roboflow Annotate](https://roboflow.com)** (web-based)\n- Free for solo use, unlimited public projects, up to 10k images\n- Drag-and-drop upload, clean bounding-box UI, good keyboard shortcuts\n- Export as \"YOLOv8\" format (one .txt per image, same filename as the image)\n- No train/val split needed on export - we handle that with cross-validation\n\n**Local alternative: [LabelImg](https://github.com/HumanSignal/labelImg)**\n- Fully offline, runs on Windows\n- `pip install labelImg` then run `labelImg` from the command line\n- Set output format to YOLO in the toolbar\n- Shortcut `W` to draw a box, `D` for next image\n\n## Dataset structure this notebook expects\n\nJust dump all your images and label files into two flat folders per ring type.\nNo train/val subdirectory needed - the k-fold split is done automatically.\n\n```\ndatasets/\n  ice/\n    images/    <- all your .png or .jpg screenshots, mixed together\n    labels/    <- matching YOLO .txt files (same filename as the image)\n  metallic/\n    images/\n    labels/\n  rocky/\n    images/\n    labels/\n  unified/     <- optional: copy of all images+labels from all ring types\n    images/\n    labels/\n```\n\nEach label .txt has one line per core:\n`0 <cx> <cy> <w> <h>` - class 0 = core, then normalized center-x, center-y,\nwidth, height (all 0.0-1.0). Both Roboflow and LabelImg produce this format.\n\n## Strategy\n\n- One model per ring type (ice, metallic, rocky) because each ring type has\n  exactly one asteroid shape that can contain a core, and you mine one ring\n  type per session so only one model needs to be loaded at a time.\n- A \"unified\" model trained on all types combined is also trained for comparison.\n- Three YOLOv8 sizes (nano, small, medium) are compared per ring type.\n- K-fold cross-validation gives reliable metric estimates even with small\n  datasets - especially useful early on when you have fewer than 200 images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install dependencies\n\nRun once. Restart the kernel after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n\npackages = [\n    \"ultralytics>=8.2.0\",\n    \"opencv-python>=4.9.0\",\n    \"pyyaml>=6.0.1\",\n    \"matplotlib>=3.9.0\",\n    \"scikit-learn>=1.5.0\",\n    \"pandas>=2.2.0\",\n    \"Pillow>=10.3.0\",\n    \"onnx>=1.16.0\",\n    \"onnxruntime-gpu>=1.18.0\",\n]\n\nsubprocess.run(\n    [sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\"] + packages,\n    check=True,\n)\nprint(\"All packages installed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nimport sys\nimport json\nimport math\nimport shutil\nimport csv\nimport yaml\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pathlib import Path\nfrom datetime import datetime\nfrom collections import defaultdict\n\nimport torch\nfrom ultralytics import YOLO\nfrom sklearn.model_selection import KFold\n\nprint(f\"Python:  {sys.version}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_properties(0)\n    vram_gb = gpu.total_memory / 1024 ** 3\n    print(f\"GPU: {gpu.name}\")\n    print(f\"VRAM: {vram_gb:.1f} GB\")\n    if vram_gb < 6:\n        print(\"WARNING: less than 6 GB VRAM - reduce BATCH_SIZE to 8 if training crashes\")\nelse:\n    print(\"WARNING: no GPU found. Training on CPU will be very slow.\")\n    print(\"Reinstall PyTorch with CUDA support:\")\n    print(\"  pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n\nAll tunable settings live here. Edit before running training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- MAIN CONFIG - edit this before running ----\n\n# Ring types to train. Remove any type you don't have data for yet.\nRING_TYPES = [\"ice\", \"metallic\", \"rocky\", \"unified\"]\n\n# Model sizes to compare per ring type.\n# n = fastest/smallest, s = good balance, m = best accuracy but slower inference.\n# All three fit on RTX 3070 8GB at batch=16.\nMODEL_SIZES = [\"yolov8n\", \"yolov8s\", \"yolov8m\"]\n\n# Number of cross-validation folds.\n# 5 is a solid default. Use 3 if you have fewer than ~30 images per ring type.\n# Each fold trains one full model and uses 1/k of the data as validation.\nK_FOLDS = 5\n\n# Training hyperparameters\nIMG_SIZE    = 640   # YOLO standard input size\nEPOCHS      = 100   # max epochs per fold (early stopping usually kicks in earlier)\nBATCH_SIZE  = 16    # reduce to 8 if you get CUDA out-of-memory errors\nPATIENCE    = 20    # stop early if val mAP doesn't improve for this many epochs\n\n# Confidence threshold for inference (0.0 - 1.0)\n# 0.4 is a good starting point. Tune based on real-game performance.\nCONF_THRESHOLD = 0.4\n\n# We only detect one class: a core asteroid.\nCLASS_NAMES = [\"core\"]\n\n# Folder layout\nDATASET_ROOT = Path(\"datasets\")\nRUNS_ROOT    = Path(\"runs\")\nEXPORTS_ROOT = Path(\"exports\")\nCV_TMP_ROOT  = Path(\"cv_tmp\")   # temporary per-fold datasets, cleaned up after training\n\nprint(\"Config loaded.\")\nprint(f\"Ring types: {RING_TYPES}\")\nprint(f\"Model sizes: {MODEL_SIZES}\")\nprint(f\"K-folds: {K_FOLDS}\")\nprint(f\"Device: {'GPU - ' + torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create dataset folder structure\n\nRun once. Then drop all your screenshots and matching label files into the folders it prints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_structure():\n    \"\"\"Create flat image and label folders for each ring type.\"\"\"\n    for ring_type in RING_TYPES:\n        (DATASET_ROOT / ring_type / \"images\").mkdir(parents=True, exist_ok=True)\n        (DATASET_ROOT / ring_type / \"labels\").mkdir(parents=True, exist_ok=True)\n\n    RUNS_ROOT.mkdir(exist_ok=True)\n    EXPORTS_ROOT.mkdir(exist_ok=True)\n    CV_TMP_ROOT.mkdir(exist_ok=True)\n\n    print(\"Folder structure created. Drop your files here:\\n\")\n    for ring_type in RING_TYPES:\n        print(f\"  datasets/{ring_type}/images/  <- all screenshots (.png or .jpg, no subfolders)\")\n        print(f\"  datasets/{ring_type}/labels/  <- matching YOLO .txt label files\")\n        print()\n    print(\"No train/val split needed - k-fold handles that automatically.\")\n\n\ncreate_dataset_structure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. COCO JSON to YOLO format converter\n\nOnly needed if your labeling tool exported COCO JSON. Roboflow can export YOLO directly - use that and skip this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coco_to_yolo(coco_json_path, output_labels_dir):\n    \"\"\"\n    Convert a COCO-format annotations JSON to per-image YOLO .txt label files.\n\n    COCO: one big JSON, bounding boxes as pixel-space (x, y, w, h) top-left origin.\n    YOLO: one .txt per image, normalized center cx cy w h on a 0-1 scale.\n\n    Args:\n        coco_json_path: path to _annotations.coco.json\n        output_labels_dir: folder where .txt files will be written\n    \"\"\"\n    output_labels_dir = Path(output_labels_dir)\n    output_labels_dir.mkdir(parents=True, exist_ok=True)\n\n    with open(coco_json_path) as f:\n        coco = json.load(f)\n\n    cat_map = {}\n    for cat in coco[\"categories\"]:\n        if cat[\"name\"] in CLASS_NAMES:\n            cat_map[cat[\"id\"]] = CLASS_NAMES.index(cat[\"name\"])\n\n    if not cat_map:\n        print(f\"WARNING: no COCO categories match CLASS_NAMES {CLASS_NAMES}\")\n        print(f\"Found in file: {[c['name'] for c in coco['categories']]}\")\n        return 0\n\n    images = {img[\"id\"]: img for img in coco[\"images\"]}\n\n    anns_by_image = defaultdict(list)\n    for ann in coco[\"annotations\"]:\n        if ann[\"category_id\"] in cat_map:\n            anns_by_image[ann[\"image_id\"]].append(ann)\n\n    converted = 0\n    for img_id, anns in anns_by_image.items():\n        img_info = images[img_id]\n        W = img_info[\"width\"]\n        H = img_info[\"height\"]\n        img_name = Path(img_info[\"file_name\"]).stem\n\n        lines = []\n        for ann in anns:\n            cls  = cat_map[ann[\"category_id\"]]\n            x, y, w, h = ann[\"bbox\"]\n            cx = (x + w / 2) / W\n            cy = (y + h / 2) / H\n            nw = w / W\n            nh = h / H\n            lines.append(f\"{cls} {cx:.6f} {cy:.6f} {nw:.6f} {nh:.6f}\")\n\n        out_path = output_labels_dir / f\"{img_name}.txt\"\n        out_path.write_text(\"\\n\".join(lines))\n        converted += 1\n\n    print(f\"Converted {converted} images -> {output_labels_dir}\")\n    return converted\n\n\n# Example:\n# convert_coco_to_yolo(\n#     coco_json_path=\"my_export/_annotations.coco.json\",\n#     output_labels_dir=\"datasets/ice/labels\",\n# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validate datasets\n\nRun after adding screenshots and labels. Catches missing files, bad label format, and warns if K_FOLDS is too high for the number of images you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset(ring_type):\n    \"\"\"\n    Check a ring type dataset for problems before training.\n    Returns (stats dict, list of issue strings). Empty issues = all clear.\n    \"\"\"\n    img_dir = DATASET_ROOT / ring_type / \"images\"\n    lbl_dir = DATASET_ROOT / ring_type / \"labels\"\n\n    images = list(img_dir.glob(\"*.png\")) + list(img_dir.glob(\"*.jpg\"))\n    labels = list(lbl_dir.glob(\"*.txt\"))\n\n    img_stems = {p.stem for p in images}\n    lbl_stems = {p.stem for p in labels}\n\n    issues = []\n\n    missing_labels = img_stems - lbl_stems\n    orphan_labels  = lbl_stems - img_stems\n\n    if missing_labels:\n        issues.append(\n            f\"{len(missing_labels)} image(s) have no label file: \"\n            + str(sorted(missing_labels)[:5])\n        )\n    if orphan_labels:\n        issues.append(\n            f\"{len(orphan_labels)} label file(s) have no matching image: \"\n            + str(sorted(orphan_labels)[:5])\n        )\n\n    total_cores = 0\n    bad_lines = 0\n    for lbl in labels:\n        for line in lbl.read_text().strip().splitlines():\n            line = line.strip()\n            if not line:\n                continue\n            parts = line.split()\n            if len(parts) != 5:\n                bad_lines += 1\n                continue\n            try:\n                cls = int(parts[0])\n                cx, cy, w, h = float(parts[1]), float(parts[2]), float(parts[3]), float(parts[4])\n                if not (0 <= cx <= 1 and 0 <= cy <= 1 and 0 < w <= 1 and 0 < h <= 1):\n                    bad_lines += 1\n                elif cls == 0:\n                    total_cores += 1\n            except ValueError:\n                bad_lines += 1\n\n    if bad_lines:\n        issues.append(f\"{bad_lines} malformed label line(s)\")\n\n    paired = len(img_stems & lbl_stems)\n    min_needed = K_FOLDS * 2\n    if paired < min_needed:\n        issues.append(\n            f\"only {paired} labeled images but K_FOLDS={K_FOLDS} needs at least {min_needed}. \"\n            f\"Add more data or reduce K_FOLDS in config.\"\n        )\n\n    stats = {\n        \"images\":  len(images),\n        \"paired\":  paired,\n        \"cores\":   total_cores,\n        \"avg_cores_per_image\": round(total_cores / paired, 2) if paired else 0,\n    }\n\n    print(f\"\\n{'=' * 44}\")\n    print(f\"  {ring_type.upper()} dataset\")\n    print(f\"{'=' * 44}\")\n    print(f\"  Images:              {stats['images']}\")\n    print(f\"  Paired (img+label):  {stats['paired']}\")\n    print(f\"  Total core boxes:    {stats['cores']}\")\n    print(f\"  Avg cores/image:     {stats['avg_cores_per_image']}\")\n\n    if issues:\n        print(\"  ISSUES:\")\n        for iss in issues:\n            print(f\"    - {iss}\")\n    else:\n        print(\"  All checks passed.\")\n\n    return stats, issues\n\n\nfor ring_type in RING_TYPES:\n    validate_dataset(ring_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Preview labeled images\n\nSanity check - confirm your labels look correct before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_labels(ring_type, n=4):\n    \"\"\"\n    Show the first n labeled images with bounding boxes overlaid.\n    Use this to confirm labels imported correctly.\n    \"\"\"\n    img_dir = DATASET_ROOT / ring_type / \"images\"\n    lbl_dir = DATASET_ROOT / ring_type / \"labels\"\n\n    images = sorted(list(img_dir.glob(\"*.png\")) + list(img_dir.glob(\"*.jpg\")))[:n]\n\n    if not images:\n        print(f\"No images found in {img_dir}\")\n        return\n\n    cols = min(len(images), 4)\n    rows = math.ceil(len(images) / cols)\n    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5 * rows))\n\n    if rows == 1 and cols == 1:\n        axes = [axes]\n    elif rows == 1 or cols == 1:\n        axes = list(axes.flat)\n    else:\n        axes = [ax for row in axes for ax in row]\n\n    for i, ax in enumerate(axes):\n        if i >= len(images):\n            ax.axis(\"off\")\n            continue\n\n        img_path = images[i]\n        img = cv2.imread(str(img_path))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        H, W = img.shape[:2]\n\n        lbl_path = lbl_dir / (img_path.stem + \".txt\")\n        if lbl_path.exists():\n            for line in lbl_path.read_text().strip().splitlines():\n                parts = line.split()\n                if len(parts) != 5:\n                    continue\n                _, cx, cy, w, h = (float(p) for p in parts)\n                x1 = int((cx - w / 2) * W)\n                y1 = int((cy - h / 2) * H)\n                x2 = int((cx + w / 2) * W)\n                y2 = int((cy + h / 2) * H)\n                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n                cv2.putText(img, \"core\", (x1, max(0, y1 - 5)),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n        ax.imshow(img)\n        ax.set_title(img_path.name, fontsize=8)\n        ax.axis(\"off\")\n\n    plt.suptitle(f\"{ring_type} - label preview\", fontsize=12)\n    plt.tight_layout()\n    plt.show()\n\n\n# Change ring_type to whichever dataset you want to inspect\npreview_labels(\"ice\", n=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. K-fold cross-validation helpers\n\nThese functions build temporary per-fold datasets using hard links (fast, no\nextra disk space), write a data.yaml for each fold, then clean up afterward.\n\n**Why k-fold instead of a fixed train/val split?**\n\nWith a fixed split, one metric number depends heavily on which images happened\nto land in the val set by luck. With k-fold you train k models, each validated\non a different 1/k slice of your data, and average the results. This gives a\nmuch more reliable accuracy estimate - especially important with small datasets\nearly in the project. The best fold's weights are kept as the deployed model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paired_samples(ring_type):\n    \"\"\"\n    Return a sorted list of (image_path, label_path) pairs for a ring type.\n    Only includes images that have a matching label file.\n    \"\"\"\n    img_dir = DATASET_ROOT / ring_type / \"images\"\n    lbl_dir = DATASET_ROOT / ring_type / \"labels\"\n\n    images = sorted(list(img_dir.glob(\"*.png\")) + list(img_dir.glob(\"*.jpg\")))\n    pairs  = []\n    for img_path in images:\n        lbl_path = lbl_dir / (img_path.stem + \".txt\")\n        if lbl_path.exists():\n            pairs.append((img_path, lbl_path))\n    return pairs\n\n\ndef build_fold_dataset(pairs_train, pairs_val, fold_dir):\n    \"\"\"\n    Build a temporary YOLO dataset for one fold using hard links.\n    Hard links point to the same inode as the original - no extra disk space used.\n    Falls back to a regular file copy if the filesystem doesn't support hard links.\n\n    Args:\n        pairs_train: list of (img_path, lbl_path) for training\n        pairs_val:   list of (img_path, lbl_path) for validation\n        fold_dir:    Path to write this fold's dataset into\n    \"\"\"\n    def link_or_copy(src, dst):\n        dst.parent.mkdir(parents=True, exist_ok=True)\n        if dst.exists():\n            dst.unlink()\n        try:\n            os.link(src, dst)\n        except OSError:\n            shutil.copy2(src, dst)\n\n    for split, pairs in [(\"train\", pairs_train), (\"val\", pairs_val)]:\n        for img_path, lbl_path in pairs:\n            link_or_copy(img_path, fold_dir / \"images\" / split / img_path.name)\n            link_or_copy(lbl_path, fold_dir / \"labels\" / split / lbl_path.name)\n\n\ndef write_fold_yaml(fold_dir):\n    \"\"\"Write the data.yaml that points YOLO at this fold's train/val folders.\"\"\"\n    yaml_path = fold_dir / \"data.yaml\"\n    config = {\n        \"path\":  str(fold_dir.resolve()),\n        \"train\": \"images/train\",\n        \"val\":   \"images/val\",\n        \"nc\":    len(CLASS_NAMES),\n        \"names\": CLASS_NAMES,\n    }\n    with open(yaml_path, \"w\") as f:\n        yaml.dump(config, f, default_flow_style=False)\n    return yaml_path\n\n\ndef cleanup_fold(fold_dir):\n    \"\"\"Delete the temporary fold dataset folder after training.\"\"\"\n    if fold_dir.exists():\n        shutil.rmtree(fold_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_fold(yaml_path, run_name, model_size, n_train, n_val):\n    \"\"\"\n    Train one YOLO model on a prepared fold dataset.\n\n    Args:\n        yaml_path:  Path to the fold's data.yaml\n        run_name:   name used as the output folder name under RUNS_ROOT\n        model_size: e.g. \"yolov8s\"\n        n_train:    number of training images (for logging)\n        n_val:      number of validation images\n\n    Returns:\n        (weights_path str, metrics dict)\n    \"\"\"\n    print(f\"  Training {run_name}  (train={n_train}, val={n_val})\")\n\n    model = YOLO(f\"{model_size}.pt\")\n\n    results = model.train(\n        data=str(yaml_path),\n        epochs=EPOCHS,\n        imgsz=IMG_SIZE,\n        batch=BATCH_SIZE,\n        patience=PATIENCE,\n        device=0 if torch.cuda.is_available() else \"cpu\",\n        project=str(RUNS_ROOT),\n        name=run_name,\n\n        # -- Augmentation tuned for Elite Dangerous screenshots --\n        # Asteroids spin at all angles, so heavy rotation is always valid\n        degrees=45,\n        # Horizontal flip is always geometrically valid in space\n        fliplr=0.5,\n        # Vertical flip is valid too - no fixed orientation in zero-g\n        flipud=0.3,\n        # Cores appear at different distances = different apparent sizes\n        scale=0.5,\n        # Mosaic helps the model handle cluttered asteroid fields\n        mosaic=1.0,\n        close_mosaic=10,\n        # Brightness/saturation shift for star-lit vs shadow-side lighting\n        hsv_v=0.4,\n        hsv_s=0.5,\n        # Small hue shift - ring types differ but not wildly\n        hsv_h=0.015,\n        # Game renders are sharp - skip blur augmentations\n        blur=False,\n        median_blur=0.0,\n\n        # Don't save intermediate checkpoints to save disk space\n        save_period=0,\n        verbose=False,\n    )\n\n    rd = results.results_dict\n    weights = RUNS_ROOT / run_name / \"weights\" / \"best.pt\"\n    metrics = {\n        \"mAP50\":     float(rd.get(\"metrics/mAP50(B)\",     0)),\n        \"mAP50_95\":  float(rd.get(\"metrics/mAP50-95(B)\",  0)),\n        \"precision\": float(rd.get(\"metrics/precision(B)\", 0)),\n        \"recall\":    float(rd.get(\"metrics/recall(B)\",    0)),\n    }\n    print(f\"    -> mAP50={metrics['mAP50']:.3f}  P={metrics['precision']:.3f}  R={metrics['recall']:.3f}\")\n    return str(weights), metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train models with k-fold cross-validation\n\nFor each combination of ring type x model size this will:\n1. Split the full dataset into K folds\n2. Train K models, each validated on a different held-out chunk\n3. Average metrics across all folds for a stable accuracy estimate\n4. Keep the weights from the best-scoring fold as the final model\n\nRough timing on RTX 3070: yolov8n ~5-10 min/fold, yolov8s ~10-20 min/fold,\nyolov8m ~20-40 min/fold. Total = folds x model sizes x ring types x per-fold time.\nEarly stopping usually cuts it well short of the full EPOCHS count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kfold(ring_type, model_size):\n    \"\"\"\n    Run K-fold cross-validation for one ring type + model size combination.\n\n    Returns a result dict with averaged metrics and path to the best fold's\n    weights, or None if the ring type doesn't have enough data.\n    \"\"\"\n    pairs = get_paired_samples(ring_type)\n\n    if len(pairs) < K_FOLDS * 2:\n        print(f\"Skipping {ring_type}/{model_size}: only {len(pairs)} paired samples \"\n              f\"(need at least {K_FOLDS * 2} for {K_FOLDS} folds).\")\n        return None\n\n    print(f\"\\n{'=' * 52}\")\n    print(f\"  {ring_type.upper()} / {model_size}  ({len(pairs)} images, {K_FOLDS} folds)\")\n    print(f\"{'=' * 52}\")\n\n    kf        = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n    pairs_arr = np.array(pairs, dtype=object)\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n    fold_results = []\n\n    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(pairs_arr)):\n        fold_num    = fold_idx + 1\n        pairs_train = pairs_arr[train_idx].tolist()\n        pairs_val   = pairs_arr[val_idx].tolist()\n\n        fold_dir = CV_TMP_ROOT / f\"{ring_type}_{model_size}_fold{fold_num}\"\n        run_name = f\"{ring_type}_{model_size}_fold{fold_num}_{timestamp}\"\n\n        print(f\"\\nFold {fold_num}/{K_FOLDS}\")\n        build_fold_dataset(pairs_train, pairs_val, fold_dir)\n        yaml_path = write_fold_yaml(fold_dir)\n\n        weights, metrics = train_one_fold(\n            yaml_path, run_name, model_size, len(pairs_train), len(pairs_val)\n        )\n        fold_results.append({\"fold\": fold_num, \"weights\": weights, **metrics})\n\n        cleanup_fold(fold_dir)\n\n    # average metrics across folds\n    avg = {\n        \"mAP50\":     float(np.mean([r[\"mAP50\"]    for r in fold_results])),\n        \"mAP50_95\":  float(np.mean([r[\"mAP50_95\"] for r in fold_results])),\n        \"precision\": float(np.mean([r[\"precision\"] for r in fold_results])),\n        \"recall\":    float(np.mean([r[\"recall\"]    for r in fold_results])),\n        \"mAP50_std\": float(np.std( [r[\"mAP50\"]    for r in fold_results])),\n    }\n\n    best_fold = max(fold_results, key=lambda r: r[\"mAP50\"])\n\n    print(f\"\\nAverage across {K_FOLDS} folds:\")\n    print(f\"  mAP50={avg['mAP50']:.3f} (+/-{avg['mAP50_std']:.3f})  \"\n          f\"mAP50-95={avg['mAP50_95']:.3f}  \"\n          f\"P={avg['precision']:.3f}  R={avg['recall']:.3f}\")\n    print(f\"  Best fold: {best_fold['fold']} (mAP50={best_fold['mAP50']:.3f})\")\n    print(f\"  Best weights: {best_fold['weights']}\")\n\n    return {\n        \"best_weights\": best_fold[\"weights\"],\n        \"best_fold\":    best_fold[\"fold\"],\n        \"fold_results\": fold_results,\n        **avg,\n    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all ring type x model size combinations.\n# Skips any ring type that doesn't have enough data yet.\n\nall_results = {}\n\nfor ring_type in RING_TYPES:\n    stats, issues = validate_dataset(ring_type)\n    if issues:\n        print(f\"Skipping {ring_type} - fix dataset issues first:\")\n        for iss in issues:\n            print(f\"  {iss}\")\n        continue\n\n    if stats[\"paired\"] < K_FOLDS * 2:\n        print(f\"Skipping {ring_type} - not enough images yet ({stats['paired']}).\")\n        continue\n\n    all_results[ring_type] = {}\n\n    for model_size in MODEL_SIZES:\n        result = train_kfold(ring_type, model_size)\n        if result is not None:\n            all_results[ring_type][model_size] = result\n\nresults_path = RUNS_ROOT / \"all_results.json\"\nRUNS_ROOT.mkdir(exist_ok=True)\nwith open(results_path, \"w\") as f:\n    json.dump(all_results, f, indent=2)\n\nprint(\"\\nAll training runs complete. Results saved to:\", results_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_comparison_table():\n    \"\"\"\n    Print a formatted table of all trained model metrics averaged across folds.\n    The +/- column shows standard deviation across folds - lower means more stable.\n    \"\"\"\n    results_path = RUNS_ROOT / \"all_results.json\"\n    if not results_path.exists():\n        print(\"No results yet - run training first.\")\n        return {}\n\n    with open(results_path) as f:\n        results = json.load(f)\n\n    header = (\n        f\"{'Ring':<12} {'Model':<12} {'mAP50':>8} {'  +/-':>6} \"\n        f\"{'mAP50-95':>10} {'Precision':>10} {'Recall':>8}\"\n    )\n    print(header)\n    print(\"-\" * len(header))\n\n    best_per_ring = {}\n    for ring_type, models in results.items():\n        best_map = -1\n        for model_size, m in models.items():\n            marker = \"\"\n            if m[\"mAP50\"] > best_map:\n                best_map = m[\"mAP50\"]\n                best_per_ring[ring_type] = model_size\n                marker = \"  <-- best\"\n            print(\n                f\"{ring_type:<12} {model_size:<12} \"\n                f\"{m['mAP50']:>8.3f} {m.get('mAP50_std', 0):>6.3f} \"\n                f\"{m['mAP50_95']:>10.3f} \"\n                f\"{m['precision']:>10.3f} {m['recall']:>8.3f}\"\n                + marker\n            )\n        print()\n\n    return best_per_ring\n\n\nbest_models = show_comparison_table()\nprint(\"Best model per ring type:\", best_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Plot per-fold metrics\n\nShows how much variance there is between folds. High variance usually means you need more labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fold_metrics(ring_type, model_size):\n    \"\"\"\n    Bar chart of mAP50, precision, and recall per fold.\n    The dashed line marks the mean mAP50 across all folds.\n    High variance between bars = model is sensitive to which images land in val,\n    which usually means you need more training data.\n    \"\"\"\n    results_path = RUNS_ROOT / \"all_results.json\"\n    if not results_path.exists():\n        print(\"No results found.\")\n        return\n\n    with open(results_path) as f:\n        all_r = json.load(f)\n\n    model_data = all_r.get(ring_type, {}).get(model_size)\n    if not model_data:\n        print(f\"No data for {ring_type}/{model_size}\")\n        return\n\n    fold_results = model_data.get(\"fold_results\", [])\n    if not fold_results:\n        print(\"No per-fold results stored.\")\n        return\n\n    folds  = [f\"Fold {r['fold']}\" for r in fold_results]\n    map50s = [r[\"mAP50\"]     for r in fold_results]\n    precs  = [r[\"precision\"] for r in fold_results]\n    recs   = [r[\"recall\"]    for r in fold_results]\n\n    x = np.arange(len(folds))\n    w = 0.25\n\n    fig, ax = plt.subplots(figsize=(10, 5))\n    ax.bar(x - w, map50s, w, label=\"mAP50\",    color=\"steelblue\")\n    ax.bar(x,     precs,  w, label=\"Precision\", color=\"seagreen\")\n    ax.bar(x + w, recs,   w, label=\"Recall\",    color=\"tomato\")\n\n    mean_map = np.mean(map50s)\n    ax.axhline(mean_map, color=\"steelblue\", linestyle=\"--\", linewidth=1.2,\n               label=f\"mean mAP50={mean_map:.3f}\")\n\n    ax.set_xticks(x)\n    ax.set_xticklabels(folds)\n    ax.set_ylim(0, 1.05)\n    ax.set_ylabel(\"Score\")\n    ax.set_title(f\"{ring_type} / {model_size} - metrics per fold\")\n    ax.legend()\n    ax.grid(True, axis=\"y\", alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n\nplot_fold_metrics(\"ice\", \"yolov8s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Plot training curves for a specific fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(ring_type, model_size, fold=None):\n    \"\"\"\n    Plot loss and mAP curves from a training run's results.csv.\n\n    Args:\n        ring_type:  e.g. \"ice\"\n        model_size: e.g. \"yolov8s\"\n        fold:       fold number to plot, or None to plot the best fold\n    \"\"\"\n    results_path = RUNS_ROOT / \"all_results.json\"\n    if not results_path.exists():\n        print(\"No results found.\")\n        return\n\n    with open(results_path) as f:\n        all_r = json.load(f)\n\n    model_data = all_r.get(ring_type, {}).get(model_size)\n    if not model_data:\n        print(f\"No data for {ring_type}/{model_size}\")\n        return\n\n    if fold is None:\n        fold = model_data[\"best_fold\"]\n        print(f\"Plotting best fold: {fold}\")\n\n    weights_path = next(\n        (r[\"weights\"] for r in model_data[\"fold_results\"] if r[\"fold\"] == fold),\n        None,\n    )\n    if weights_path is None:\n        print(f\"Fold {fold} not found in results.\")\n        return\n\n    run_dir  = Path(weights_path).parent.parent\n    csv_path = run_dir / \"results.csv\"\n    if not csv_path.exists():\n        print(f\"results.csv not found at {csv_path}\")\n        return\n\n    epochs, box_loss, cls_loss, map50 = [], [], [], []\n    with open(csv_path) as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            row = {k.strip(): v.strip() for k, v in row.items()}\n            epochs.append(int(row.get(\"epoch\", 0)))\n            box_loss.append(float(row.get(\"train/box_loss\", 0) or row.get(\"train/dfl_loss\", 0) or 0))\n            cls_loss.append(float(row.get(\"train/cls_loss\", 0) or 0))\n            map50.append(float(row.get(\"metrics/mAP50(B)\", 0) or 0))\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n    ax1.plot(epochs, box_loss, label=\"box loss\")\n    ax1.plot(epochs, cls_loss, label=\"cls loss\")\n    ax1.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.set_title(f\"{ring_type} / {model_size} fold {fold} - training loss\")\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    ax2.plot(epochs, map50, color=\"green\", label=\"mAP50\")\n    ax2.set_xlabel(\"Epoch\")\n    ax2.set_ylabel(\"mAP50\")\n    ax2.set_title(f\"{ring_type} / {model_size} fold {fold} - validation mAP50\")\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n\nplot_training_curves(\"ice\", \"yolov8s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test inference on a screenshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cores(image_path, ring_type, model_size=None, conf=CONF_THRESHOLD):\n    \"\"\"\n    Run the trained detector on a single screenshot and show the result.\n\n    Args:\n        image_path: path to a .png or .jpg screenshot\n        ring_type:  which ring type model to load (\"ice\", \"metallic\", \"rocky\", \"unified\")\n        model_size: e.g. \"yolov8s\", or None to auto-pick the best one\n        conf:       confidence threshold\n    \"\"\"\n    results_path = RUNS_ROOT / \"all_results.json\"\n    if not results_path.exists():\n        print(\"No trained models found. Run training first.\")\n        return\n\n    with open(results_path) as f:\n        all_r = json.load(f)\n\n    ring_models = all_r.get(ring_type, {})\n    if not ring_models:\n        print(f\"No trained model for ring type '{ring_type}'.\")\n        return\n\n    if model_size is None:\n        model_size = max(ring_models, key=lambda m: ring_models[m][\"mAP50\"])\n        print(f\"Auto-selected best model: {model_size} (mAP50={ring_models[model_size]['mAP50']:.3f})\")\n\n    weights = ring_models.get(model_size, {}).get(\"best_weights\", \"\")\n    if not weights or not Path(weights).exists():\n        print(f\"Weights file not found: {weights}\")\n        return\n\n    model = YOLO(weights)\n    img   = cv2.imread(str(image_path))\n    if img is None:\n        print(f\"Could not load image: {image_path}\")\n        return\n\n    preds     = model.predict(img, conf=conf, verbose=False)[0]\n    annotated = preds.plot()\n\n    plt.figure(figsize=(14, 8))\n    plt.imshow(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB))\n    plt.title(\n        f\"{ring_type} cores | {model_size} | \"\n        f\"{len(preds.boxes)} detection(s) | conf>={conf}\"\n    )\n    plt.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\n    print(f\"Detections: {len(preds.boxes)}\")\n    for box in preds.boxes:\n        xyxy = [round(v, 1) for v in box.xyxy[0].tolist()]\n        print(f\"  core | conf={float(box.conf):.3f} | bbox={xyxy}\")\n\n    return preds\n\n\n# Usage:\n# detect_cores(\"screenshot.png\", ring_type=\"ice\")\n# detect_cores(\"screenshot.png\", ring_type=\"metallic\", model_size=\"yolov8s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. One-button retrain\n\nDrop new labeled screenshots into the dataset folder and call `retrain()`.\nIt re-validates, runs the full k-fold loop using the best model size from the\nlast run, and updates the results table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain(ring_type=\"all\", model_size=\"best\"):\n    \"\"\"\n    Retrain after adding new labeled screenshots.\n    Drop new images + label files into datasets/<ring_type>/images/ and labels/,\n    then call this.\n\n    Args:\n        ring_type:  \"all\" retrains every ring type, or pass one e.g. \"ice\"\n        model_size: \"best\" reuses the size that scored highest last time,\n                    or pass a specific size like \"yolov8s\"\n    \"\"\"\n    targets = RING_TYPES if ring_type == \"all\" else [ring_type]\n\n    results_path = RUNS_ROOT / \"all_results.json\"\n    prev_results = {}\n    if results_path.exists():\n        with open(results_path) as f:\n            prev_results = json.load(f)\n\n    for rt in targets:\n        stats, issues = validate_dataset(rt)\n        if issues:\n            print(f\"Dataset issues for '{rt}' - fix before retraining:\")\n            for iss in issues:\n                print(f\"  {iss}\")\n            continue\n\n        if stats[\"paired\"] < K_FOLDS * 2:\n            print(f\"Skipping '{rt}' - not enough images ({stats['paired']}). \"\n                  f\"Need at least {K_FOLDS * 2}.\")\n            continue\n\n        if model_size == \"best\":\n            ring_prev = prev_results.get(rt, {})\n            size = max(ring_prev, key=lambda m: ring_prev[m][\"mAP50\"]) if ring_prev else \"yolov8s\"\n            print(f\"Using best model size from previous run: {size}\")\n        else:\n            size = model_size\n\n        result = train_kfold(rt, size)\n        if result is not None:\n            if rt not in prev_results:\n                prev_results[rt] = {}\n            prev_results[rt][size] = result\n\n    with open(results_path, \"w\") as f:\n        json.dump(prev_results, f, indent=2)\n\n    print(\"\\nRetrain complete!\")\n    show_comparison_table()\n\n\n# Examples:\n# retrain()                      # retrain all ring types, best model size each\n# retrain(\"ice\")                 # retrain only ice, auto-pick best model size\n# retrain(\"ice\", \"yolov8m\")      # force a specific model size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Export best models to ONNX\n\nONNX lets the companion app run inference without a full PyTorch install and enables DirectML/CUDA acceleration via ONNX Runtime on Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_best_models():\n    \"\"\"\n    Export the best fold's weights for each ring type to ONNX format.\n    Output goes to exports/<ring_type>_best.onnx\n    \"\"\"\n    results_path = RUNS_ROOT / \"all_results.json\"\n    if not results_path.exists():\n        print(\"No trained models to export.\")\n        return\n\n    with open(results_path) as f:\n        all_r = json.load(f)\n\n    EXPORTS_ROOT.mkdir(exist_ok=True)\n\n    for ring_type, models in all_r.items():\n        if not models:\n            continue\n\n        best_size    = max(models, key=lambda m: models[m][\"mAP50\"])\n        best_weights = models[best_size][\"best_weights\"]\n\n        if not Path(best_weights).exists():\n            print(f\"Weights missing for {ring_type}/{best_size}: {best_weights}\")\n            continue\n\n        print(f\"Exporting {ring_type} ({best_size}, mAP50={models[best_size]['mAP50']:.3f})...\")\n        model    = YOLO(best_weights)\n        exported = model.export(\n            format=\"onnx\",\n            imgsz=IMG_SIZE,\n            simplify=True,\n            opset=17,\n            dynamic=False,\n        )\n\n        dest    = EXPORTS_ROOT / f\"{ring_type}_best.onnx\"\n        shutil.copy(exported, dest)\n        size_mb = dest.stat().st_size / 1024 / 1024\n        print(f\"  -> {dest}  ({size_mb:.1f} MB)\")\n\n    print(\"\\nExport complete. ONNX models are in:\", EXPORTS_ROOT)\n\n\nexport_best_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Batch inference on a folder of screenshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_detect(screenshots_folder, ring_type, output_folder=None, conf=CONF_THRESHOLD):\n    \"\"\"\n    Run detection on every image in a folder and save annotated copies.\n\n    Args:\n        screenshots_folder: folder with .png/.jpg screenshots\n        ring_type:          which ring type model to use\n        output_folder:      where to save annotated images (default: screenshots_folder/detected)\n        conf:               confidence threshold\n    \"\"\"\n    screenshots_folder = Path(screenshots_folder)\n    if output_folder is None:\n        output_folder = screenshots_folder / \"detected\"\n    output_folder = Path(output_folder)\n    output_folder.mkdir(exist_ok=True)\n\n    results_path = RUNS_ROOT / \"all_results.json\"\n    if not results_path.exists():\n        print(\"No trained models. Run training first.\")\n        return\n\n    with open(results_path) as f:\n        all_r = json.load(f)\n\n    ring_models = all_r.get(ring_type, {})\n    if not ring_models:\n        print(f\"No model for ring type '{ring_type}'\")\n        return\n\n    best_size    = max(ring_models, key=lambda m: ring_models[m][\"mAP50\"])\n    best_weights = ring_models[best_size][\"best_weights\"]\n    model        = YOLO(best_weights)\n\n    images = sorted(\n        list(screenshots_folder.glob(\"*.png\"))\n        + list(screenshots_folder.glob(\"*.jpg\"))\n    )\n    print(f\"Running {best_size} ({ring_type}) on {len(images)} images...\")\n\n    total_cores = 0\n    for img_path in images:\n        img   = cv2.imread(str(img_path))\n        preds = model.predict(img, conf=conf, verbose=False)[0]\n        cv2.imwrite(str(output_folder / img_path.name), preds.plot())\n        total_cores += len(preds.boxes)\n\n    print(f\"Done. {total_cores} total core detections across {len(images)} screenshots.\")\n    print(f\"Annotated images saved to: {output_folder}\")\n\n\n# Usage:\n# batch_detect(\n#     screenshots_folder=r\"C:/Users/YourName/Pictures/Frontier Developments/Elite Dangerous\",\n#     ring_type=\"ice\",\n# )\n"
   ]
  }
 ]
}